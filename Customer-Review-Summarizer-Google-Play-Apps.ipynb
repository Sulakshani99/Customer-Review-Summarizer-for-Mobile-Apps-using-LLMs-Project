{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kgjzO3L_LlB6",
    "outputId": "ce0ae7ab-f9ee-4585-ab2e-65e020997f30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (4.56.1)\n",
      "Requirement already satisfied: datasets in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (4.0.0)\n",
      "Requirement already satisfied: evaluate in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (0.4.5)\n",
      "Requirement already satisfied: accelerate in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (1.10.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (0.2.1)\n",
      "Requirement already satisfied: filelock in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from transformers) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: xxhash in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: psutil in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from accelerate) (2.8.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: setuptools in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: rouge_score in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from rouge_score) (2.3.1)\n",
      "Requirement already satisfied: nltk in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from rouge_score) (2.3.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: click in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from nltk->rouge_score) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from nltk->rouge_score) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from nltk->rouge_score) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages (from nltk->rouge_score) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 1. Install & Import Libraries\n",
    "# ==============================\n",
    "!pip install -U transformers datasets evaluate accelerate sentencepiece\n",
    "!pip install rouge_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, EarlyStoppingCallback, DataCollatorForSeq2Seq\n",
    "import evaluate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fN3ydBJrLu7P",
    "outputId": "bc7e9e53-08c2-4aa4-fde9-0510e898eadb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/data/googleplaystore_user_reviews.csv\n",
      "Sample data:\n",
      "                      App                                  Translated_Review  \\\n",
      "0  10 Best Foods for You  I like eat delicious food. That's I'm cooking ...   \n",
      "1  10 Best Foods for You    This help eating healthy exercise regular basis   \n",
      "2  10 Best Foods for You         Works great especially going grocery store   \n",
      "3  10 Best Foods for You                                       Best idea us   \n",
      "4  10 Best Foods for You                                           Best way   \n",
      "\n",
      "  Sentiment  \n",
      "0  Positive  \n",
      "1  Positive  \n",
      "2  Positive  \n",
      "3  Positive  \n",
      "4  Positive  \n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 2. Load Dataset\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "DATA_PATH = os.path.abspath(os.path.join(os.pardir, 'data', 'googleplaystore_user_reviews.csv'))\n",
    "print('Loading dataset from:', DATA_PATH)\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Keep only needed columns (App, Review, Sentiment)\n",
    "df = df[[\"App\", \"Translated_Review\", \"Sentiment\"]]\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "print(\"Sample data:\\n\", df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "c0bVYLVYL8rL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial rows: 37427\n",
      "Removed short (<10 chars): 3203\n",
      "Removed duplicates: 7435\n",
      "Remaining rows: 26789\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Translated_Review', 'summary', '__index_level_0__'],\n",
      "        num_rows: 21431\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['Translated_Review', 'summary', '__index_level_0__'],\n",
      "        num_rows: 5358\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 3. Preprocessing (enhanced cleaning)\n",
    "# ==============================\n",
    "# If dataset not loaded (cell 2 skipped), load it here safely\n",
    "import os, re\n",
    "try:\n",
    "    df\n",
    "except NameError:\n",
    "    DATA_PATH = os.path.abspath(os.path.join(os.pardir, 'data', 'googleplaystore_user_reviews.csv'))\n",
    "    print('(Re)loading dataset inside preprocessing from:', DATA_PATH)\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    df = df[[\"App\", \"Translated_Review\", \"Sentiment\"]].dropna().reset_index(drop=True)\n",
    "\n",
    "print(f\"Initial rows: {len(df)}\")\n",
    "\n",
    "# Basic text cleaning function\n",
    "_clean_url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "_non_alpha_pattern = re.compile(r'[^a-z\\s]')\n",
    "_multi_space_pattern = re.compile(r'\\s+')\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = str(text).lower().strip()\n",
    "    text = _clean_url_pattern.sub(' ', text)\n",
    "    # Keep only letters + space\n",
    "    text = _non_alpha_pattern.sub(' ', text)\n",
    "    # Collapse repeated chars (e.g., cooooool -> coool) simple heuristic\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "    text = _multi_space_pattern.sub(' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Backup raw review\n",
    "if 'raw_review' not in df.columns:\n",
    "    df['raw_review'] = df['Translated_Review']\n",
    "\n",
    "# Apply cleaning\n",
    "df['Translated_Review'] = df['Translated_Review'].apply(clean_text)\n",
    "\n",
    "# Drop empty / very short after cleaning\n",
    "before_len = len(df)\n",
    "min_len = 10\n",
    "mask = df['Translated_Review'].str.len() >= min_len\n",
    "removed_short = before_len - mask.sum()\n",
    "df = df[mask]\n",
    "\n",
    "# Remove duplicates on cleaned text\n",
    "before_dedup = len(df)\n",
    "df = df.drop_duplicates(subset=['Translated_Review'])\n",
    "removed_dup = before_dedup - len(df)\n",
    "\n",
    "print(f\"Removed short (<{min_len} chars): {removed_short}\")\n",
    "print(f\"Removed duplicates: {removed_dup}\")\n",
    "print(f\"Remaining rows: {len(df)}\")\n",
    "\n",
    "# Rebuild summaries from cleaned text (using sentiment)\n",
    "def build_summary(row):\n",
    "    txt = row['Translated_Review']\n",
    "    if row['Sentiment'] == 'Positive':\n",
    "        return f\"Positive feedback: {txt}\"\n",
    "    elif row['Sentiment'] == 'Negative':\n",
    "        return f\"Problem: {txt}\"\n",
    "    else:\n",
    "        return f\"Neutral comment: {txt}\"\n",
    "\n",
    "df['summary'] = df.apply(build_summary, axis=1)\n",
    "\n",
    "# Create HuggingFace dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dataset = Dataset.from_pandas(df[['Translated_Review', 'summary']])\n",
    "\n",
    "# Train/validation split\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset['train'].shuffle(seed=42),\n",
    "    'validation': dataset['test'].shuffle(seed=42)\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326,
     "referenced_widgets": [
      "5cfd31766a934c578b62b891a836500d",
      "58268c97e0644fc1a568e5702e116028",
      "387af6ad11f44001b7523913861f570d",
      "c285e5ed0e48436fab20ce6b7aa4588e",
      "a2b3a6c4a711466e8f39f780228ac554",
      "e3865225ab254132b6058c7b790824b5",
      "0ca74ddf1bbd445489f978347ef0989f",
      "653f21e9eab443c8bc404aab4a4677e5",
      "de820d723cdc46dbaed20894b9131be5",
      "04630089defc42efb25408e106869926",
      "4e244b5f61e14713a25243f3cb4f8e89",
      "d45f03336d544703aaeef0799f00e97e",
      "fb40717576bc4d1d962644c7cb82ee79",
      "99e861d7394246ceb857408755e71d52",
      "fc6e617c08b148d1b8cf90530b67fc7c",
      "fe60d6ff5259411280f2690b75b0cbc2",
      "709f2946b7354f9dade6b4ae81da96d5",
      "c4173c1abcd447d88422fdd5535f3cea",
      "316cc7242c844bb288b8fe0292d3e113",
      "0bde9ebb876043c4a42dfd7e11288bbc",
      "df7c43c1eb4e43ce934895e772fb3d76",
      "33764fc5c7de4dd3aacf9637cb066c83",
      "56679a36e96e40a584dd9b463fc92ed2",
      "24f6106dfc74467596379cfc2432d6db",
      "7b1d92efa26a42b897601194d032dc18",
      "ee986d5e1071420b8a9f38aae7199576",
      "ab950ce8a7884d7190b0eca5dff97a0d",
      "04333f12f4654045b1b0e42717cf42b7",
      "758ffaa17c44431092749cff9a92deec",
      "9c8302396d1c4b21ace3016ad694a1e2",
      "be2432abb4f14f1c8738e3f12b868866",
      "c7818e7fabb64119afa28d76878f1b17",
      "d1b5ccbaac3648ac823104269ab1ed7f",
      "816822b41a4e4dfca05d56f26adf417d",
      "56d115c86ec04bcf9024e8c570bbbc7e",
      "f2c4099d567e4e15a607e94f5516c8c7",
      "87142a13db694c9e87c9429baf3d9a0a",
      "8c195248fcf84980ab795ba63b58b20c",
      "06b75bb53d28485794babe3bbb1b02dc",
      "cfbba8554000456ab856e9be1eb5a5c2",
      "06aba812ce044c86958e9ab0859224c0",
      "403a777bea9c4eafae3f1bb369d557c8",
      "93bfbb2eb49a4305b8e96c4edcf881c8",
      "517a73eda7774b00bfa6b80586c4a851",
      "9c05752eaf8d48439b81de53f64b295d",
      "cc61309445ef4c9180b892415ac2ad96",
      "55461054a7214ccc9f670e2310f2dd26",
      "f3ba980d246a428e9e891434ff9358cc",
      "7116a0ad2deb4ef4bb2b8fd1787432ee",
      "2ad2ed2298fc4db9b968de1322c342da",
      "dac72fd25316466995e17eebdef02742",
      "6683f328eb85489190d9987de7795296",
      "3ae0e01614f84605a807ea6cae9a8381",
      "7c173d6002e24debb28f67e296d7f1c1",
      "f4d6049fef3a466fa8c482934c1243ef"
     ]
    },
    "id": "tlrnOYXPL_7A",
    "outputId": "dd5e4c9e-fdd2-4010-f54e-5010551944a8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ba0f3882444fb284bb6533d34df47e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21431 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:4007: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae173a84d1f5469ea0312fe03e1c66b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==============================\n",
    "# 4. Tokenization\n",
    "# ==============================\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "max_input_length = 256\n",
    "max_target_length = 64\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\"summarize: \" + doc for doc in examples[\"Translated_Review\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    "    label_ids = labels[\"input_ids\"]\n",
    "    # Replace pad token ids in labels with -100\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    label_ids = [[(lid if lid != pad_token_id else -100) for lid in seq] for seq in label_ids]\n",
    "    model_inputs[\"labels\"] = label_ids\n",
    "    return model_inputs\n",
    "\n",
    "# (Optional) Subsample for quick experiment\n",
    "#small_train = dataset[\"train\"].select(range(512)) if dataset[\"train\"].num_rows > 512 else dataset[\"train\"]\n",
    "#small_val = dataset[\"validation\"].select(range(128)) if dataset[\"validation\"].num_rows > 128 else dataset[\"validation\"]\n",
    "big_train = dataset[\"train\"]\n",
    "big_val = dataset[\"validation\"]\n",
    "\n",
    "working_dataset = DatasetDict({\"train\": big_train, \"validation\": big_val})\n",
    "\n",
    "tokenized_dataset = working_dataset.map(preprocess_function, batched=True, remove_columns=[\"Translated_Review\", \"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "f46bba77758c4336a6ba26c1d659b787",
      "b5765f81572d4b8199f8510d4427a55f",
      "e1736427ca6d4ad887ff5e9372819b79",
      "98080670683c4a26a7a44d00f71fdb00",
      "60a7bb70698747bba668eb72b6026b02",
      "dc0a5e3c2c664028ae059d2ea5ecd99c",
      "48d84c5c9a1b40639b644d75224fd1c6",
      "c08b45d3812340e8bf4deeb225b973b5",
      "28db633d8e1142b693e749ec62ce126a",
      "c123eef68e42427aa47c0b6424363e9f",
      "1abd1752030f4cbba2d059dc0689be0e",
      "2272cecf9e714ceab1186dd9ea811404",
      "2a213450041b4651b8a0a39a57205663",
      "a5c0347c081b4d9fb771d93fa10a2c63",
      "d7155b2edfe14c8f87f0691bb5db2bb9",
      "cd4ecbe10fee42349aa11e2fa117719a",
      "090639f80d5a49bc810bbe1c037b44a1",
      "9a7c9d52e1db4acfbdc4d4a036f4743e",
      "8eddee38fd734e00971f3208cde9ed22",
      "9f759eb130564255b751a3efad60ce7a",
      "da58ee18c02f4bed8890f3784942f572",
      "aef51e2e37f84e78b6529280cccd860d",
      "c758d07be4f5405e8e3d31573cb8cf7e",
      "925f16cf7417444c91eb82f68ecf5075",
      "7d783b6049534edc950be6dafe6d8c4f",
      "1bef6286598d401fa4df8c432dbe484e",
      "d037d570f875482e98ea898a2734b8f0",
      "010948da568f46fe99169d5f0379f93e",
      "9097cbfa99654e22a2d604a86d244311",
      "fe53d8854010425c85fe3cd2d8c57280",
      "c95b1d4a560e4f44b97f5be9580e81bd",
      "cce6ebd5dea74d7d8333fbdac2e6ebd9",
      "3aa7b433b7be454f80f68c191501919e"
     ]
    },
    "id": "GhhNkWHmMDf5",
    "outputId": "053ad421-544d-4eb6-db85-9c15f59aaa2b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# 5. Load Model\n",
    "# ==============================\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21431"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].num_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5358"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"].num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "id": "WLcIgPHPMLS3",
    "outputId": "4d897e3f-b171-4f80-ab67-a81b901298d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.56.1\n",
      "TrainingArguments init params: ['self', 'output_dir', 'overwrite_output_dir', 'do_train', 'do_eval', 'do_predict', 'eval_strategy', 'prediction_loss_only', 'per_device_train_batch_size', 'per_device_eval_batch_size', 'per_gpu_train_batch_size', 'per_gpu_eval_batch_size', 'gradient_accumulation_steps', 'eval_accumulation_steps', 'eval_delay', 'torch_empty_cache_steps', 'learning_rate', 'weight_decay', 'adam_beta1', 'adam_beta2', 'adam_epsilon', 'max_grad_norm', 'num_train_epochs', 'max_steps', 'lr_scheduler_type', 'lr_scheduler_kwargs', 'warmup_ratio', 'warmup_steps', 'log_level', 'log_level_replica', 'log_on_each_node', 'logging_dir', 'logging_strategy', 'logging_first_step', 'logging_steps', 'logging_nan_inf_filter', 'save_strategy', 'save_steps', 'save_total_limit', 'save_safetensors', 'save_on_each_node', 'save_only_model', 'restore_callback_states_from_checkpoint', 'no_cuda', 'use_cpu', 'use_mps_device', 'seed', 'data_seed', 'jit_mode_eval', 'use_ipex', 'bf16', 'fp16', 'fp16_opt_level', 'half_precision_backend', 'bf16_full_eval', 'fp16_full_eval', 'tf32', 'local_rank', 'ddp_backend', 'tpu_num_cores', 'tpu_metrics_debug', 'debug', 'dataloader_drop_last', 'eval_steps', 'dataloader_num_workers', 'dataloader_prefetch_factor', 'past_index', 'run_name', 'disable_tqdm', 'remove_unused_columns', 'label_names', 'load_best_model_at_end', 'metric_for_best_model', 'greater_is_better', 'ignore_data_skip', 'fsdp', 'fsdp_min_num_params', 'fsdp_config', 'fsdp_transformer_layer_cls_to_wrap', 'accelerator_config', 'parallelism_config', 'deepspeed', 'label_smoothing_factor', 'optim', 'optim_args', 'adafactor', 'group_by_length', 'length_column_name', 'report_to', 'ddp_find_unused_parameters', 'ddp_bucket_cap_mb', 'ddp_broadcast_buffers', 'dataloader_pin_memory', 'dataloader_persistent_workers', 'skip_memory_metrics', 'use_legacy_prediction_loop', 'push_to_hub', 'resume_from_checkpoint', 'hub_model_id', 'hub_strategy', 'hub_token', 'hub_private_repo', 'hub_always_push', 'hub_revision', 'gradient_checkpointing', 'gradient_checkpointing_kwargs', 'include_inputs_for_metrics', 'include_for_metrics', 'eval_do_concat_batches', 'fp16_backend', 'push_to_hub_model_id', 'push_to_hub_organization', 'push_to_hub_token', 'mp_parameters', 'auto_find_batch_size', 'full_determinism', 'torchdynamo', 'ray_scope', 'ddp_timeout', 'torch_compile', 'torch_compile_backend', 'torch_compile_mode', 'include_tokens_per_second', 'include_num_input_tokens_seen', 'neftune_noise_alpha', 'optim_target_modules', 'batch_eval_metrics', 'eval_on_start', 'use_liger_kernel', 'liger_kernel_config', 'eval_use_gather_object', 'average_tokens_across_devices']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/llqj6lcn4n7c0xglj39dv4xh0000gn/T/ipykernel_4965/193934746.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4020' max='4020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4020/4020 1:45:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.982400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.935100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.843600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.662200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.424900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.181300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.031800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.957200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.904800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.879500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.861200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.842100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.832400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.816200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.804300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.798400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.790700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.783200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.785200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.777100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.767000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.764600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.758100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.755300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.750200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.751600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.743800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.742700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.740600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.739000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.735200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.732400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.731100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.726500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.728900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.723300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.723000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.723500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.714200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.713400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.714700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.713900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.715600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.708700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.709000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.707300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>1.708400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.703200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>1.702800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.704200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>1.699000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.700600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>1.700100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.701800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>1.698100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.697400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>1.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.695800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>1.699700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.694500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>1.695900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.696500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>1.690900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.692800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>1.690100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.695300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>1.693200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.690900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>1.691800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.687100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>1.691100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.690500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>1.688800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.688500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>1.686100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.688200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>1.686500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.689500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>1.684800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.684600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss: 1.8061\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1339' max='1339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1339/1339 02:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after training: 1.6801\n",
      "Saved model and tokenizer to ./t5_finetuned_reviews\n",
      "Saved model and tokenizer to ./t5_finetuned_reviews\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 7. Training Setup (3 epochs, no manual early stopping)\n",
    "# ==============================\n",
    "import transformers\n",
    "print('Transformers version:', transformers.__version__)\n",
    "from inspect import signature\n",
    "print('TrainingArguments init params:', list(signature(TrainingArguments.__init__).parameters.keys()))\n",
    "\n",
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "OUTPUT_DIR = './t5_finetuned_reviews_model'\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_strategy=\"no\",  \n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.2,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8, \n",
    "    dataloader_drop_last=True,\n",
    "    label_smoothing_factor=0.1,\n",
    "    num_train_epochs=5,  \n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    optim=\"adamw_torch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=False,\n",
    "    logging_strategy='steps',\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=200,\n",
    "    report_to=[],\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 8. Train (no manual loop)\n",
    "# ==============================\n",
    "import math, os\n",
    "\n",
    "train_output = trainer.train()\n",
    "final_train_loss = getattr(train_output, 'training_loss', None)\n",
    "if final_train_loss is not None and not math.isnan(final_train_loss):\n",
    "    print(f\"Final training loss (from Trainer): {final_train_loss:.4f}\")\n",
    "\n",
    "# Also print the last logged training loss from step logs\n",
    "last_step_loss = None\n",
    "for entry in reversed(getattr(trainer.state, 'log_history', [])):\n",
    "    if 'loss' in entry:\n",
    "        last_step_loss = entry['loss']\n",
    "        break\n",
    "if last_step_loss is not None:\n",
    "    print(f\"Last logged training loss (step): {last_step_loss:.4f}\")\n",
    "\n",
    "# Evaluate once after training\n",
    "eval_output = trainer.evaluate()\n",
    "val_loss = float(eval_output.get('eval_loss', float('nan')))\n",
    "if not math.isnan(val_loss):\n",
    "    print(f\"Validation loss after training: {val_loss:.4f}\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Saved model and tokenizer to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhSe4O-iMQ6f"
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 9. Training Loss Summary (recent steps and per-epoch means)\n",
    "# ==============================\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "try:\n",
    "    logs = getattr(trainer.state, 'log_history', [])\n",
    "    train_step_losses = [x for x in logs if 'loss' in x and 'epoch' in x]\n",
    "    # Print last 10 step losses\n",
    "    print(\"Last 10 training loss logs:\")\n",
    "    for row in train_step_losses[-10:]:\n",
    "        step = row.get('step')\n",
    "        loss = row.get('loss')\n",
    "        epoch = row.get('epoch')\n",
    "        print(f\"step={step:>6}  epoch={epoch:>4.1f}  loss={loss:.4f}\")\n",
    "\n",
    "    # Compute per-epoch mean training loss from step logs\n",
    "    from collections import defaultdict\n",
    "    buckets = defaultdict(list)\n",
    "    for r in train_step_losses:\n",
    "        ep = f\"{float(r.get('epoch', 0)):.0f}\"\n",
    "        buckets[ep].append(float(r['loss']))\n",
    "    if buckets:\n",
    "        print(\"\\nPer-epoch mean training loss (approx from step logs):\")\n",
    "        for ep in sorted(buckets, key=lambda k: int(k)):\n",
    "            vals = buckets[ep]\n",
    "            print(f\"epoch {ep}: mean loss {sum(vals)/len(vals):.4f} (n={len(vals)})\")\n",
    "except Exception as e:\n",
    "    print(\"Could not read trainer logs:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "bnPFAPiGLbaJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== One summary per sentiment for 10 Best Foods for You ===\n",
      "\n",
      "Positive Summary:\n",
      " i like eat delicious food that s i m cooking food myself case best foods helps lot also best before shelf life - this help eating healthy exercise regular basis - works great especially going grocery store - best idea us - useful information the amount spelling errors questions validity information shared once fixed stars given - thank you great app add arthritis eyes immunity kidney \n",
      "\n",
      "Negative Summary:\n",
      " no recipe book unable recipe book - waste time it needs internet time n ask calls information - faltu plz waste ur time - crap doesn t work - boring i thought actually just texts that s it too poor old texts \n",
      "\n",
      "Neutral Summary:\n",
      " looking forward app - it helpful site it help foods get - god health - i found lot wealth form health - this helpful - doesn t work zero - this starr download - i like was helpful \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 10. App-level Summarization (one summary per sentiment)\n",
    "# ==============================\n",
    "import os\n",
    "import re as _re\n",
    "import torch\n",
    "\n",
    "# Use this saved model by default\n",
    "DEFAULT_SAVED_MODEL_DIR = \\\n",
    "    \"/Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/notebook/t5_finetuned_reviews\"\n",
    "\n",
    "# Map dataset sentiments to the prefixes used during training\n",
    "_SENTIMENT_PREFIX = {\n",
    "    \"Positive\": \"Positive feedback:\",\n",
    "    \"Negative\": \"Problem:\",\n",
    "    \"Neutral\":  \"Neutral comment:\",\n",
    "}\n",
    "\n",
    "# Decide which model/tokenizer to use (auto-load saved model if available and not loaded yet)\n",
    "def _get_handles():\n",
    "    global val_model, val_tokenizer\n",
    "    if 'val_model' in globals() and 'val_tokenizer' in globals():\n",
    "        mdl = val_model\n",
    "        tok = val_tokenizer\n",
    "    else:\n",
    "        if os.path.isdir(DEFAULT_SAVED_MODEL_DIR):\n",
    "            from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "            tok = T5Tokenizer.from_pretrained(DEFAULT_SAVED_MODEL_DIR)\n",
    "            mdl = T5ForConditionalGeneration.from_pretrained(DEFAULT_SAVED_MODEL_DIR)\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "            mdl.to(device)\n",
    "            val_model = mdl\n",
    "            val_tokenizer = tok\n",
    "        else:\n",
    "            mdl = model\n",
    "            tok = tokenizer\n",
    "    dev = next(mdl.parameters()).device\n",
    "    return mdl, tok, dev\n",
    "\n",
    "# Generate a summary with an optional forced prefix; strip the prefix for display\n",
    "def _strip_prefix(txt: str) -> str:\n",
    "    return _re.sub(r\"^(Positive feedback:|Problem:|Neutral comment:)\\s*\", \"\", str(txt)).strip()\n",
    "\n",
    "\n",
    "def _generate_prefixed(text: str, forced_prefix: str | None = None, max_length: int = 60, num_beams: int = 4,\n",
    "                       strip_prefix: bool = True) -> str:\n",
    "    mdl, tok, dev = _get_handles()\n",
    "    enc = tok(\"summarize: \" + text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    enc = {k: v.to(dev) for k, v in enc.items()}\n",
    "\n",
    "    out_text = None\n",
    "\n",
    "    # If we have the helper from Cell 13 and a saved model, use it\n",
    "    if forced_prefix and 'generate_with_forced_prefix' in globals() and 'val_model' in globals():\n",
    "        try:\n",
    "            out_text, _, _ = generate_with_forced_prefix(text, prefixes=[forced_prefix], max_length=max_length, num_beams=num_beams)\n",
    "        except Exception:\n",
    "            out_text = None\n",
    "\n",
    "    if out_text is None:\n",
    "        # Fallback: seed decoder with the forced prefix (if provided), else plain generate\n",
    "        gen_kwargs = dict(max_length=max_length, num_beams=num_beams, early_stopping=True)\n",
    "        if forced_prefix:\n",
    "            dec_ids = tok(forced_prefix, return_tensors='pt', add_special_tokens=False).input_ids.to(dev)\n",
    "            gen_kwargs[\"decoder_input_ids\"] = dec_ids\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outs = mdl.generate(**enc, **gen_kwargs)\n",
    "        except RuntimeError as e:\n",
    "            # MPS fallback\n",
    "            if dev.type == 'mps':\n",
    "                enc_cpu = {k: v.to('cpu') for k, v in enc.items()}\n",
    "                with torch.no_grad():\n",
    "                    outs = mdl.to('cpu').generate(**enc_cpu, **gen_kwargs)\n",
    "                mdl.to(dev)\n",
    "            else:\n",
    "                raise\n",
    "        out_text = tok.decode(outs[0], skip_special_tokens=True)\n",
    "\n",
    "    return _strip_prefix(out_text) if strip_prefix else out_text\n",
    "\n",
    "\n",
    "def summarize_app_by_sentiment(app_name: str, per_group_limit: int = 50, strip_prefix: bool = True) -> dict:\n",
    "    \"\"\"Return one summary per sentiment for the given app.\n",
    "    per_group_limit: max number of reviews per sentiment to include (to keep prompt length reasonable).\n",
    "    \"\"\"\n",
    "    assert 'df' in globals(), \"Dataframe 'df' not found; run data loading cells first.\"\n",
    "\n",
    "    app_reviews = df[df[\"App\"] == app_name][[\"Translated_Review\", \"Sentiment\"]].dropna()\n",
    "    results = {}\n",
    "    for sent in [\"Positive\", \"Negative\", \"Neutral\"]:\n",
    "        subset = app_reviews[app_reviews[\"Sentiment\"] == sent][\"Translated_Review\"].astype(str).tolist()\n",
    "        if not subset:\n",
    "            results[sent] = \"\"\n",
    "            continue\n",
    "        # Take up to per_group_limit reviews, join with separators\n",
    "        texts = subset[:per_group_limit]\n",
    "        joined = \" \\n- \".join(texts)  # lightweight structuring\n",
    "        forced = _SENTIMENT_PREFIX[sent]\n",
    "        summary = _generate_prefixed(joined, forced_prefix=forced, max_length=80, num_beams=4, strip_prefix=strip_prefix)\n",
    "        results[sent] = summary\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "app_name = \"10 Best Foods for You\"\n",
    "summaries = summarize_app_by_sentiment(app_name, per_group_limit=50, strip_prefix=True)\n",
    "print(f\"=== One summary per sentiment for {app_name} ===\\n\")\n",
    "print(\"Positive Summary:\\n\", summaries.get(\"Positive\", \"\"), \"\\n\")\n",
    "print(\"Negative Summary:\\n\", summaries.get(\"Negative\", \"\"), \"\\n\")\n",
    "print(\"Neutral Summary:\\n\", summaries.get(\"Neutral\", \"\"), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved model from: /Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/notebook/t5_finetuned_reviews\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/llqj6lcn4n7c0xglj39dv4xh0000gn/T/ipykernel_4965/1028412585.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  val_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='670' max='670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [670/670 01:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Saved Model Evaluation ===\n",
      "{'eval_loss': 0.08040162175893784, 'eval_model_preparation_time': 0.0014, 'eval_runtime': 78.2963, 'eval_samples_per_second': 68.432, 'eval_steps_per_second': 8.557}\n",
      "eval_loss: 0.0804\n",
      "\n",
      "=== Sample Generations (saved model) ===\n",
      "\n",
      "[1] Original : great always however bug latest even sync launch selected wakes says syncing notification bar never syncs forever says syncing the option force close several times day for i must alternative\n",
      "    Reference: Positive feedback: great always however bug latest even sync launch selected wakes says syncing notification bar never syncs forever says syncing the option force close several times day for i must alternative\n",
      "    Predicted: Positive feedback: great always however bug latest even sync launch selected wakes says syncing notification bar never syncs forever says\n",
      "\n",
      "[2] Original : i love playing game like many others said hard get gems without spending money makes frustrating there way could get gems easily trade coins gems one problem i game customer tech support awful continue give run around i played game roughly day row collecting gems day daily reward day decided give daily reward instead put back day get gems day this happened several times me i contacted customer tech support nothing help resolve issue make right instead kept telling fault likely issue device time altered first i never mess settings phone issue i traveling different time zones want admit glitch game great game need gem opportunities better tech support\n",
      "    Reference: Problem: i love playing game like many others said hard get gems without spending money makes frustrating there way could get gems easily trade coins gems one problem i game customer tech support awful continue give run around i played game roughly day row collecting gems day daily reward day decided give daily reward instead put back day get gems day this happened several times me i contacted customer tech support nothing help resolve issue make right instead kept telling fault likely issue device time altered first i never mess settings phone issue i traveling different time zones want admit glitch game great game need gem opportunities better tech support\n",
      "    Predicted: Positive feedback: i love playing game like many others said hard get gems without spending money makes frustrating there way could get gems easily trade\n",
      "\n",
      "[3] Original : you watch ads tank you wanna know much time wasted is i math minutes ads just tank might never use thats reaching do ya really need much money this game pay cosmetics fine abuse ok but i mean minutes tank i didnt actually watch minutes ads i know ad seconds gold ak fortnite doesnt even come close this sorry mobile version activision but worse they least put effort evwn tho super duper small official game\n",
      "    Reference: Problem: you watch ads tank you wanna know much time wasted is i math minutes ads just tank might never use thats reaching do ya really need much money this game pay cosmetics fine abuse ok but i mean minutes tank i didnt actually watch minutes ads i know ad seconds gold ak fortnite doesnt even come close this sorry mobile version activision but worse they least put effort evwn tho super duper small official game\n",
      "    Predicted: Problem: you watch ads tank you wanna know much time wasted is i math minutes ads just tank might never use thats reaching do\n",
      "\n",
      "[4] Original : this really make feel creative if really person likes colour combinations stuff\n",
      "    Reference: Positive feedback: this really make feel creative if really person likes colour combinations stuff\n",
      "    Predicted: Positive feedback: this really make feel creative if really person likes colour combinations stuff\n",
      "\n",
      "[5] Original : good place keep memories\n",
      "    Reference: Positive feedback: good place keep memories\n",
      "    Predicted: Positive feedback: good place keep memories\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 11. Validate a Saved Model (eval loss + sample generations)\n",
    "# ==============================\n",
    "import os\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "SAVED_MODEL_DIR = \"/Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/notebook/t5_finetuned_reviews\"\n",
    "print(\"Loading saved model from:\", SAVED_MODEL_DIR)\n",
    "\n",
    "# Load tokenizer/model\n",
    "val_tokenizer = T5Tokenizer.from_pretrained(SAVED_MODEL_DIR)\n",
    "val_model = T5ForConditionalGeneration.from_pretrained(SAVED_MODEL_DIR)\n",
    "\n",
    "# Move to device\n",
    "_device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "val_model.to(_device)\n",
    "print(\"Using device:\", _device)\n",
    "\n",
    "# Data collator and eval-only Trainer\n",
    "val_data_collator = DataCollatorForSeq2Seq(tokenizer=val_tokenizer, model=val_model)\n",
    "\n",
    "val_args = TrainingArguments(\n",
    "    output_dir=\"./_tmp_eval_saved_model\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    dataloader_drop_last=False,\n",
    "    report_to=[],\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"no\",\n",
    ")\n",
    "\n",
    "val_trainer = Trainer(\n",
    "    model=val_model,\n",
    "    args=val_args,\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=val_tokenizer,\n",
    "    data_collator=val_data_collator,\n",
    ")\n",
    "\n",
    "# Evaluate on validation split\n",
    "eval_metrics = val_trainer.evaluate()\n",
    "print(\"\\n=== Saved Model Evaluation ===\")\n",
    "print({k: float(v) for k, v in eval_metrics.items() if isinstance(v, (int, float))})\n",
    "if \"eval_loss\" in eval_metrics:\n",
    "    print(f\"eval_loss: {float(eval_metrics['eval_loss']):.4f}\")\n",
    "\n",
    "# Qualitative check: generate summaries for a few validation samples (with MPS->CPU fallback)\n",
    "print(\"\\n=== Sample Generations (saved model) ===\")\n",
    "try:\n",
    "    src_ds = working_dataset[\"validation\"] if 'working_dataset' in globals() else dataset[\"validation\"]\n",
    "    n = min(5, src_ds.num_rows)\n",
    "    samples = src_ds.select(range(n))\n",
    "    for i in range(n):\n",
    "        text = samples[i][\"Translated_Review\"]\n",
    "        ref = samples[i][\"summary\"]\n",
    "        inputs = val_tokenizer(\"summarize: \" + text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "        try:\n",
    "            inputs = inputs.to(_device)\n",
    "            with torch.no_grad():\n",
    "                outputs = val_model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=30,\n",
    "                    min_length=5,\n",
    "                    num_beams=4,\n",
    "                    early_stopping=True,\n",
    "                )\n",
    "        except RuntimeError as e:\n",
    "            if \"Placeholder storage has not been allocated on MPS device\" in str(e) or (_device.type == 'mps'):\n",
    "                print(\"[Info] MPS generate failed; retrying on CPU for this sample...\")\n",
    "                val_model_cpu = val_model.to('cpu')\n",
    "                inputs_cpu = {k: v.to('cpu') for k, v in inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    outputs = val_model_cpu.generate(\n",
    "                        **inputs_cpu,\n",
    "                        max_length=30,\n",
    "                        min_length=5,\n",
    "                        num_beams=4,\n",
    "                        early_stopping=True,\n",
    "                    )\n",
    "            else:\n",
    "                raise\n",
    "        pred = val_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"\\n[{i+1}] Original : {text}\")\n",
    "        print(f\"    Reference: {ref}\")\n",
    "        print(f\"    Predicted: {pred}\")\n",
    "except Exception as e:\n",
    "    print(\"(Skipping sample generations) Reason:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25bdd0bf39c442b8b853f175e106419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ROUGE (saved model) ===\n",
      "rouge1: 0.8925\n",
      "rouge2: 0.8858\n",
      "rougeL: 0.8921\n",
      "rougeLsum: 0.8919\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 12. Saved Model Metrics: ROUGE + Sentiment-Prefix Accuracy\n",
    "# ==============================\n",
    "import math\n",
    "import evaluate as _eval\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "rouge = _eval.load(\"rouge\")\n",
    "\n",
    "# Use the same device/_device, tokenizer (val_tokenizer), and model (val_model) from the previous cell.\n",
    "# Generate on a subset for speed; increase MAX_SAMPLES for fuller evaluation.\n",
    "MAX_SAMPLES = 512  # adjust as needed\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "src_ds = working_dataset[\"validation\"] if 'working_dataset' in globals() else dataset[\"validation\"]\n",
    "N = src_ds.num_rows\n",
    "M = min(MAX_SAMPLES, N)\n",
    "samples = src_ds.select(range(M))\n",
    "\n",
    "preds = []\n",
    "refs = []\n",
    "\n",
    "def _move_to(d, device):\n",
    "    return {k: v.to(device) for k, v in d.items()}\n",
    "\n",
    "for start in tqdm(range(0, M, BATCH_SIZE)):\n",
    "    end = min(start + BATCH_SIZE, M)\n",
    "    batch = samples.select(range(start, end))\n",
    "    texts = [b[\"Translated_Review\"] for b in batch]\n",
    "    ref_texts = [b[\"summary\"] for b in batch]\n",
    "    inputs = val_tokenizer([\n",
    "        \"summarize: \" + t for t in texts\n",
    "    ], return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outs = val_model.generate(\n",
    "                **_move_to(inputs, _device),\n",
    "                max_length=30,\n",
    "                min_length=5,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "            )\n",
    "    except RuntimeError as e:\n",
    "        # MPS fallback per-batch\n",
    "        print(\"[Info] Generation failed on\", _device, \"; retrying batch on CPU...\")\n",
    "        with torch.no_grad():\n",
    "            outs = val_model.to('cpu').generate(\n",
    "                **_move_to(inputs, 'cpu'),\n",
    "                max_length=30,\n",
    "                min_length=5,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "            )\n",
    "        val_model.to(_device)\n",
    "    batch_preds = val_tokenizer.batch_decode(outs, skip_special_tokens=True)\n",
    "    preds.extend(batch_preds)\n",
    "    refs.extend(ref_texts)\n",
    "\n",
    "# ROUGE\n",
    "rouge_res = rouge.compute(\n",
    "    predictions=[\"\\n\".join(p.split()) for p in preds],\n",
    "    references=[\"\\n\".join(r.split()) for r in refs],\n",
    ")\n",
    "print(\"\\n=== ROUGE (saved model) ===\")\n",
    "for k in [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]:\n",
    "    v = rouge_res.get(k)\n",
    "    if isinstance(v, dict) and 'mid' in v and hasattr(v['mid'], 'fmeasure'):\n",
    "        score = v['mid'].fmeasure\n",
    "    elif hasattr(v, 'mid') and hasattr(v.mid, 'fmeasure'):\n",
    "        score = v.mid.fmeasure\n",
    "    else:\n",
    "        score = float(v)\n",
    "    print(f\"{k}: {score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sentiment F1 Scores (prefix-based) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive     0.8278    0.9255    0.8739       322\n",
      "    Negative     0.7300    0.5984    0.6577       122\n",
      "     Neutral     0.6923    0.5294    0.6000        68\n",
      "\n",
      "    accuracy                         0.7949       512\n",
      "   macro avg     0.7500    0.6844    0.7105       512\n",
      "weighted avg     0.7865    0.7949    0.7860       512\n",
      "\n",
      "micro F1: 0.794921875\n",
      "macro F1: 0.71051931697093\n",
      "weighted F1: 0.7859963450437241\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 12b. Sentiment F1 Scores (per-class + micro/macro/weighted)\n",
    "# ==============================\n",
    "import re as _re\n",
    "\n",
    "# Ensure we have predictions and references from the previous metrics cell\n",
    "assert 'preds' in globals() and 'refs' in globals() and len(preds) == len(refs) and len(refs) > 0, \\\n",
    "    \"Run the 'Saved Model Metrics' cell first to populate preds/refs.\"\n",
    "\n",
    "_allowed_prefixes = [\"Positive feedback:\", \"Problem:\", \"Neutral comment:\"]\n",
    "_label_map = {p: i for i, p in enumerate(_allowed_prefixes)}\n",
    "\n",
    "def _prefix(text: str) -> str:\n",
    "    m = _re.match(r\"^(Positive feedback:|Problem:|Neutral comment:)\", str(text).strip())\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "# Convert preds/refs into label ids\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for p, r in zip(preds, refs):\n",
    "    y_pred.append(_label_map.get(_prefix(p), -1))\n",
    "    y_true.append(_label_map.get(_prefix(r), -1))\n",
    "\n",
    "# Filter out any items where the reference label is unknown (shouldn't happen)\n",
    "filtered = [(yp, yt) for yp, yt in zip(y_pred, y_true) if yt != -1]\n",
    "if not filtered:\n",
    "    raise RuntimeError(\"No valid references for F1 computation.\")\n",
    "y_pred, y_true = zip(*filtered)\n",
    "\n",
    "print(\"\\n=== Sentiment F1 Scores (prefix-based) ===\")\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import classification_report, f1_score\n",
    "    target_names = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "    # Remap label ids to contiguous range [0..2]\n",
    "    # (They already are, but keep explicit.)\n",
    "    print(classification_report(y_true, y_pred, labels=[0,1,2], target_names=target_names, digits=4))\n",
    "    print(\"micro F1:\", f1_score(y_true, y_pred, average='micro'))\n",
    "    print(\"macro F1:\", f1_score(y_true, y_pred, average='macro'))\n",
    "    print(\"weighted F1:\", f1_score(y_true, y_pred, average='weighted'))\n",
    "except Exception as e:\n",
    "    # Minimal pure-Python fallback for F1 per class and macro/micro\n",
    "    from collections import Counter\n",
    "    import math\n",
    "\n",
    "    labels = [0,1,2]\n",
    "    # Confusion counts\n",
    "    cm = {l: {m: 0 for m in labels} for l in labels}\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt in labels and yp in labels:\n",
    "            cm[yt][yp] += 1\n",
    "\n",
    "    def precision_recall_f1(tp, fp, fn):\n",
    "        prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "        rec = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "        f1 = 2*prec*rec / (prec + rec) if (prec + rec) else 0.0\n",
    "        return prec, rec, f1\n",
    "\n",
    "    per_class_f1 = {}\n",
    "    micro_tp = micro_fp = micro_fn = 0\n",
    "\n",
    "    for c in labels:\n",
    "        tp = cm[c][c]\n",
    "        fp = sum(cm[r][c] for r in labels if r != c)\n",
    "        fn = sum(cm[c][r] for r in labels if r != c)\n",
    "        prec, rec, f1 = precision_recall_f1(tp, fp, fn)\n",
    "        per_class_f1[c] = f1\n",
    "        micro_tp += tp\n",
    "        micro_fp += fp\n",
    "        micro_fn += fn\n",
    "        name = [\"Positive\", \"Negative\", \"Neutral\"][c]\n",
    "        print(f\"{name:8s} F1: {f1:.4f} (P={prec:.4f}, R={rec:.4f})\")\n",
    "\n",
    "    macro_f1 = sum(per_class_f1.values()) / len(labels)\n",
    "    # Micro F1 equals micro precision/recall in multiclass\n",
    "    micro_prec, micro_rec, micro_f1 = precision_recall_f1(micro_tp, micro_fp, micro_fn)\n",
    "\n",
    "    # Weighted F1 by support\n",
    "    supports = Counter(y_true)\n",
    "    total = sum(supports[l] for l in labels)\n",
    "    weighted_f1 = sum(per_class_f1[l] * (supports[l] / total) for l in labels)\n",
    "\n",
    "    print(f\"macro F1:   {macro_f1:.4f}\")\n",
    "    print(f\"micro F1:   {micro_f1:.4f}\")\n",
    "    print(f\"weighted F1:{weighted_f1:.4f}\")\n",
    "    print(\"(Note: using fallback implementation; install scikit-learn for a detailed report.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58dc2c570e3f4645ba4c29d19ab4da51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tuning:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Top generation configs (by macro F1, tie-break ROUGE-L) ===\n",
      "#1 {'config': {'strategy': 'beam', 'num_beams': 1, 'max_length': 64, 'min_length': 5, 'length_penalty': 1.0, 'repetition_penalty': 1.0}, 'macro_f1': 0.6786211414066977, 'rougeL': 0.9568092082250543}\n",
      "#2 {'config': {'strategy': 'beam', 'num_beams': 1, 'max_length': 64, 'min_length': 5, 'length_penalty': 1.0, 'repetition_penalty': 1.15}, 'macro_f1': 0.6786211414066977, 'rougeL': 0.9568092082250543}\n",
      "#3 {'config': {'strategy': 'beam', 'num_beams': 1, 'max_length': 48, 'min_length': 5, 'length_penalty': 1.0, 'repetition_penalty': 1.0}, 'macro_f1': 0.6786211414066977, 'rougeL': 0.9408365027038681}\n",
      "#4 {'config': {'strategy': 'beam', 'num_beams': 1, 'max_length': 48, 'min_length': 5, 'length_penalty': 1.0, 'repetition_penalty': 1.15}, 'macro_f1': 0.6786211414066977, 'rougeL': 0.9408365027038681}\n",
      "#5 {'config': {'strategy': 'beam', 'num_beams': 1, 'max_length': 64, 'min_length': 10, 'length_penalty': 1.0, 'repetition_penalty': 1.0}, 'macro_f1': 0.6786211414066977, 'rougeL': 0.9217556514775453}\n",
      "\n",
      "BEST_GEN_CONFIG = {'strategy': 'beam', 'num_beams': 1, 'max_length': 64, 'min_length': 5, 'length_penalty': 1.0, 'repetition_penalty': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 12c. Lightweight hyperparameter search for generation\n",
    "# ==============================\n",
    "import itertools\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Use saved model/tokenizer and device from previous cells: val_model, val_tokenizer, _device\n",
    "assert 'val_model' in globals() and 'val_tokenizer' in globals() and '_device' in globals(), \\\n",
    "    \"Load the saved model first (run cell 11).\"\n",
    "\n",
    "# Reuse prefix extractor from earlier cells\n",
    "import re as _re\n",
    "\n",
    "def _prefix(text: str) -> str:\n",
    "    m = _re.match(r\"^(Positive feedback:|Problem:|Neutral comment:)\", str(text).strip())\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "label_to_id = {\"Positive feedback:\": 0, \"Problem:\": 1, \"Neutral comment:\": 2}\n",
    "\n",
    "# Small validation slice for quick tuning\n",
    "TUNE_MAX_SAMPLES = 96  # keep small for speed; increase if needed\n",
    "BATCH_SIZE = 8\n",
    "src_ds = working_dataset[\"validation\"] if 'working_dataset' in globals() else dataset[\"validation\"]\n",
    "N = src_ds.num_rows\n",
    "M = min(TUNE_MAX_SAMPLES, N)\n",
    "samples = src_ds.select(range(M))\n",
    "\n",
    "# Search space (kept small to be fast)\n",
    "search_space = []\n",
    "# Beam variants\n",
    "for num_beams in [1, 4]:\n",
    "    for max_len in [48, 64]:\n",
    "        for min_len in [5, 10]:\n",
    "            for rep_pen in [1.0, 1.15]:\n",
    "                search_space.append({\n",
    "                    \"strategy\": \"beam\",\n",
    "                    \"num_beams\": num_beams,\n",
    "                    \"max_length\": max_len,\n",
    "                    \"min_length\": min_len,\n",
    "                    \"length_penalty\": 1.0,\n",
    "                    \"repetition_penalty\": rep_pen,\n",
    "                })\n",
    "# Nucleus sampling variant\n",
    "for max_len in [48, 64]:\n",
    "    search_space.append({\n",
    "        \"strategy\": \"nucleus\",\n",
    "        \"top_p\": 0.9,\n",
    "        \"top_k\": 50,\n",
    "        \"temperature\": 1.0,\n",
    "        \"max_length\": max_len,\n",
    "        \"min_length\": 5,\n",
    "        \"repetition_penalty\": 1.05,\n",
    "    })\n",
    "\n",
    "# Metric helpers\n",
    "import evaluate as _eval\n",
    "_rouge = _eval.load(\"rouge\")\n",
    "\n",
    "def evaluate_config(cfg):\n",
    "    preds = []\n",
    "    refs = []\n",
    "    for start in range(0, M, BATCH_SIZE):\n",
    "        end = min(start + BATCH_SIZE, M)\n",
    "        batch = samples.select(range(start, end))\n",
    "        texts = [b[\"Translated_Review\"] for b in batch]\n",
    "        ref_texts = [b[\"summary\"] for b in batch]\n",
    "        enc = val_tokenizer([\n",
    "            \"summarize: \" + t for t in texts\n",
    "        ], return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "        gen_kwargs = dict(max_length=cfg[\"max_length\"], min_length=cfg[\"min_length\"], repetition_penalty=cfg.get(\"repetition_penalty\", 1.0))\n",
    "        if cfg[\"strategy\"] == \"beam\":\n",
    "            gen_kwargs.update(dict(num_beams=cfg[\"num_beams\"], early_stopping=True))\n",
    "        else:\n",
    "            gen_kwargs.update(dict(do_sample=True, top_p=cfg[\"top_p\"], top_k=cfg[\"top_k\"], temperature=cfg[\"temperature\"]))\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outs = val_model.generate(**{k: v.to(_device) for k, v in enc.items()}, **gen_kwargs)\n",
    "        except RuntimeError as e:\n",
    "            # MPS fallback\n",
    "            if _device.type == 'mps':\n",
    "                with torch.no_grad():\n",
    "                    outs = val_model.to('cpu').generate(**{k: v.to('cpu') for k, v in enc.items()}, **gen_kwargs)\n",
    "                val_model.to(_device)\n",
    "            else:\n",
    "                raise\n",
    "        batch_preds = val_tokenizer.batch_decode(outs, skip_special_tokens=True)\n",
    "        preds.extend(batch_preds)\n",
    "        refs.extend(ref_texts)\n",
    "\n",
    "    # ROUGE-L as primary text metric\n",
    "    rouge_res = _rouge.compute(\n",
    "        predictions=[\"\\n\".join(p.split()) for p in preds],\n",
    "        references=[\"\\n\".join(r.split()) for r in refs],\n",
    "    )\n",
    "    # Normalize to float\n",
    "    def _rouge_to_float(v):\n",
    "        if isinstance(v, dict) and 'mid' in v and hasattr(v['mid'], 'fmeasure'):\n",
    "            return float(v['mid'].fmeasure)\n",
    "        if hasattr(v, 'mid') and hasattr(v.mid, 'fmeasure'):\n",
    "            return float(v.mid.fmeasure)\n",
    "        return float(v)\n",
    "    rougeL = _rouge_to_float(rouge_res.get(\"rougeL\"))\n",
    "\n",
    "    # Macro F1 over sentiment prefix labels\n",
    "    y_true = [label_to_id.get(_prefix(r), -1) for r in refs]\n",
    "    y_pred = [label_to_id.get(_prefix(p), -1) for p in preds]\n",
    "    pairs = [(yt, yp) for yt, yp in zip(y_true, y_pred) if yt != -1]\n",
    "    if not pairs:\n",
    "        macro_f1 = 0.0\n",
    "    else:\n",
    "        labels = [0,1,2]\n",
    "        cm = {l: {m: 0 for m in labels} for l in labels}\n",
    "        for yt, yp in pairs:\n",
    "            if yp in labels:\n",
    "                cm[yt][yp] += 1\n",
    "        def prf(tp, fp, fn):\n",
    "            p = tp/(tp+fp) if (tp+fp) else 0.0\n",
    "            r = tp/(tp+fn) if (tp+fn) else 0.0\n",
    "            f1 = 2*p*r/(p+r) if (p+r) else 0.0\n",
    "            return p, r, f1\n",
    "        f1s = []\n",
    "        for c in labels:\n",
    "            tp = cm[c][c]\n",
    "            fp = sum(cm[r][c] for r in labels if r != c)\n",
    "            fn = sum(cm[c][r] for r in labels if r != c)\n",
    "            _, _, f1 = prf(tp, fp, fn)\n",
    "            f1s.append(f1)\n",
    "        macro_f1 = sum(f1s)/len(f1s)\n",
    "\n",
    "    return {\"macro_f1\": macro_f1, \"rougeL\": rougeL}\n",
    "\n",
    "results = []\n",
    "for cfg in tqdm(search_space, desc=\"Tuning\", leave=False):\n",
    "    metrics = evaluate_config(cfg)\n",
    "    results.append({\"config\": cfg, **metrics})\n",
    "\n",
    "# Rank: primary macro_f1 desc, then rougeL desc\n",
    "results_sorted = sorted(results, key=lambda x: (x[\"macro_f1\"], x[\"rougeL\"]), reverse=True)\n",
    "print(\"\\n=== Top generation configs (by macro F1, tie-break ROUGE-L) ===\")\n",
    "for i, r in enumerate(results_sorted[:5]):\n",
    "    print(f\"#{i+1}\", r)\n",
    "\n",
    "BEST_GEN_CONFIG = results_sorted[0][\"config\"] if results_sorted else None\n",
    "print(\"\\nBEST_GEN_CONFIG =\", BEST_GEN_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot summary:\n",
      " Positive feedback: You are a helpful assistant that writes concise, fluent summaries of app reviews. Include only the most important user points\n",
      "\n",
      "One-shot positive summary:\n",
      " Positive feedback: You are a helpful assistant that writes concise, fluent summaries of app reviews. Include only the most important user points\n",
      "\n",
      "One-shot positive summary:\n",
      " Positive feedback: You are a helpful assistant that writes concise, fluent summaries of app reviews. Include only the most important user points\n",
      "\n",
      "Few-shot negative summary:\n",
      " Problem: You are a helpful assistant that writes concise, fluent summaries of app reviews. Include only the most important user points\n",
      "\n",
      "Few-shot negative summary:\n",
      " Problem: You are a helpful assistant that writes concise, fluent summaries of app reviews. Include only the most important user points\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 12d. Prompt engineering: zero-shot / one-shot / few-shot helpers\n",
    "# ==============================\n",
    "import textwrap\n",
    "\n",
    "assert 'val_model' in globals() and 'val_tokenizer' in globals() and '_device' in globals(), \\\n",
    "    \"Load the saved model first (run cell 11).\"\n",
    "\n",
    "INSTR_DEFAULT = (\n",
    "    \"You are a helpful assistant that writes concise, fluent summaries of app reviews. \"\n",
    "    \"Include only the most important user points.\"\n",
    ")\n",
    "\n",
    "SENTIMENT_TEMPLATES = {\n",
    "    \"Positive\": \"Write a brief positive summary of the user reviews.\",\n",
    "    \"Negative\": \"Write a brief summary highlighting the problems and issues in the user reviews.\",\n",
    "    \"Neutral\":  \"Write a brief neutral summary of the user reviews.\",\n",
    "}\n",
    "\n",
    "SENTIMENT_TO_PREFIX = {\n",
    "    \"Positive\": \"Positive feedback:\",\n",
    "    \"Negative\": \"Problem:\",\n",
    "    \"Neutral\":  \"Neutral comment:\",\n",
    "}\n",
    "\n",
    "# Build a prompt with optional in-context examples\n",
    "# shots: list of dicts with keys: {instruction, input, target}\n",
    "\n",
    "def build_prompt(review_block: str,\n",
    "                 instruction: str = INSTR_DEFAULT,\n",
    "                 sentiment: str | None = None,\n",
    "                 shots: list | None = None,\n",
    "                 max_examples: int = 3) -> str:\n",
    "    parts = []\n",
    "    # Keep instruction short to avoid echoing\n",
    "    if instruction:\n",
    "        parts.append(f\"Instruction: {instruction}\")\n",
    "    if sentiment and sentiment in SENTIMENT_TEMPLATES:\n",
    "        parts.append(f\"Task: {SENTIMENT_TEMPLATES[sentiment]}\")\n",
    "    # In-context examples in training style\n",
    "    if shots:\n",
    "        for ex in shots[:max_examples]:\n",
    "            parts.append(\"Example:\\nInput: \" + ex.get('input', '').strip())\n",
    "            tgt = ex.get('target', '').strip()\n",
    "            if tgt:\n",
    "                parts.append(\"Output: \" + tgt)\n",
    "    parts.append(\"Input: \" + review_block.strip())\n",
    "    parts.append(\"Output:\")\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "# Generate with optional prompt (wrapped by the T5 \"summarize: \" prefix),\n",
    "# and seed the decoder with the sentiment prefix if provided.\n",
    "\n",
    "def generate_with_prompt(review_block: str,\n",
    "                         instruction: str = INSTR_DEFAULT,\n",
    "                         sentiment: str | None = None,\n",
    "                         shots: list | None = None,\n",
    "                         gen_config: dict | None = None) -> str:\n",
    "    prompt = build_prompt(review_block, instruction=instruction, sentiment=sentiment, shots=shots)\n",
    "    input_text = \"summarize: \" + prompt\n",
    "    enc = val_tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # Base cfg; do not set deprecated flags\n",
    "    cfg = dict(max_length=64, min_length=8, num_beams=4)\n",
    "    # Use tuned config if available\n",
    "    if 'BEST_GEN_CONFIG' in globals() and BEST_GEN_CONFIG:\n",
    "        # Map strategy flags\n",
    "        mapped = dict(BEST_GEN_CONFIG)\n",
    "        if mapped.get('strategy') == 'nucleus':\n",
    "            mapped.update(dict(do_sample=True))\n",
    "            mapped.pop('strategy', None)\n",
    "        else:\n",
    "            mapped.pop('strategy', None)\n",
    "        cfg.update({k: v for k, v in mapped.items() if k in {\n",
    "            'num_beams','max_length','min_length','length_penalty','repetition_penalty','do_sample','top_p','top_k','temperature'\n",
    "        }})\n",
    "    if gen_config:\n",
    "        cfg.update(gen_config)\n",
    "\n",
    "    # Seed decoder with the sentiment-specific prefix if provided\n",
    "    gen_kwargs = dict(cfg)\n",
    "    seed_ids = None\n",
    "    if sentiment and sentiment in SENTIMENT_TO_PREFIX:\n",
    "        prefix_text = SENTIMENT_TO_PREFIX[sentiment]\n",
    "        seed_ids = val_tokenizer(prefix_text, return_tensors='pt', add_special_tokens=False).input_ids\n",
    "        gen_kwargs['decoder_input_ids'] = seed_ids.to(_device)\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outs = val_model.generate(**{k: v.to(_device) for k, v in enc.items()}, **gen_kwargs)\n",
    "    except RuntimeError as e:\n",
    "        if _device.type == 'mps':\n",
    "            with torch.no_grad():\n",
    "                # Ensure seed ids on cpu when falling back\n",
    "                di = gen_kwargs.get('decoder_input_ids')\n",
    "                if di is not None:\n",
    "                    gen_kwargs['decoder_input_ids'] = di.to('cpu')\n",
    "                outs = val_model.to('cpu').generate(**{k: v.to('cpu') for k, v in enc.items()}, **gen_kwargs)\n",
    "            val_model.to(_device)\n",
    "        else:\n",
    "            raise\n",
    "    text = val_tokenizer.decode(outs[0], skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "# Tiny demo\n",
    "try:\n",
    "    # Build a small review block\n",
    "    demo_texts = [\n",
    "        \"The app is super fast and the UI looks great.\",\n",
    "        \"Crashes sometimes when opening settings.\",\n",
    "        \"Notifications are delayed after the last update.\"\n",
    "    ]\n",
    "    review_block = \" - \" + \"\\n - \".join(demo_texts)\n",
    "\n",
    "    # Zero-shot (no sentiment seeding)\n",
    "    z = generate_with_prompt(review_block, sentiment=None)\n",
    "    print(\"Zero-shot summary:\\n\", z)\n",
    "\n",
    "    # One-shot Positive (with decoder seeding)\n",
    "    one_shot = [{\n",
    "        \"input\": \" - love the clean interface and quick responses\\n - great performance overall\",\n",
    "        \"target\": \"Positive feedback: clean UI and fast performance appreciated by users.\"\n",
    "    }]\n",
    "    o = generate_with_prompt(review_block, sentiment=\"Positive\", shots=one_shot)\n",
    "    print(\"\\nOne-shot positive summary:\\n\", o)\n",
    "\n",
    "    # Few-shot Negative (with decoder seeding)\n",
    "    few_shots = [\n",
    "        {\n",
    "            \"input\": \" - app is intuitive and smooth\\n - great features for daily use\",\n",
    "            \"target\": \"Positive feedback: intuitive design and useful features for daily tasks.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \" - crashes on launch\\n - bug in login screen\",\n",
    "            \"target\": \"Problem: frequent crashes and login bug impact usability.\"\n",
    "        }\n",
    "    ]\n",
    "    f = generate_with_prompt(review_block, sentiment=\"Negative\", shots=few_shots)\n",
    "    print(\"\\nFew-shot negative summary:\\n\", f)\n",
    "except Exception as e:\n",
    "    print(\"(Prompt demo skipped)\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7707aa4778d482381dd815df3875453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train gen:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe0cd835a2848ddadf4d7b3d2acfa02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation gen:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/llqj6lcn4n7c0xglj39dv4xh0000gn/T/ipykernel_4965/676325525.py:75: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_train_eval = Trainer(model=val_model, args=_tmp_args, eval_dataset=tokenized_dataset['train'], tokenizer=val_tokenizer, data_collator=_data_collator)\n",
      "/var/folders/7g/llqj6lcn4n7c0xglj39dv4xh0000gn/T/ipykernel_4965/676325525.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_val_eval   = Trainer(model=val_model, args=_tmp_args, eval_dataset=tokenized_dataset['validation'], tokenizer=val_tokenizer, data_collator=_data_collator)\n",
      "/Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2679' max='2679' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2679/2679 06:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='670' max='670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [670/670 01:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Train vs Validation (ROUGE and eval_loss) ===\n",
      "Train ROUGE: {'rouge1': 0.9037, 'rouge2': 0.8972, 'rougeL': 0.9036, 'rougeLsum': 0.9036}\n",
      "Valid ROUGE: {'rouge1': 0.8925, 'rouge2': 0.8858, 'rougeL': 0.8921, 'rougeLsum': 0.8919}\n",
      "Train eval_loss: 0.0800\n",
      "Valid eval_loss: 0.0804\n",
      "\n",
      "Delta (train - valid) ROUGE-L: 0.0115\n",
      "Delta (valid - train) eval_loss: 0.0004\n",
      "[Note] No strong overfitting signal based on these thresholds.\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 12e. Train vs Validation: ROUGE + eval_loss comparison\n",
    "# ==============================\n",
    "import math\n",
    "import re as _re\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate as _eval\n",
    "\n",
    "# Preconditions\n",
    "assert 'val_model' in globals() and 'val_tokenizer' in globals() and '_device' in globals(), \\\n",
    "    \"Load the saved model first (run the validation cell).\"\n",
    "assert 'tokenized_dataset' in globals() and 'working_dataset' in globals(), \\\n",
    "    \"Ensure tokenized_dataset and working_dataset are defined (run earlier cells).\"\n",
    "\n",
    "_rouge_metric = _eval.load(\"rouge\")\n",
    "\n",
    "def _move_to(d, device):\n",
    "    return {k: v.to(device) for k, v in d.items()}\n",
    "\n",
    "def _rouge_to_float(v):\n",
    "    if isinstance(v, dict) and 'mid' in v and hasattr(v['mid'], 'fmeasure'):\n",
    "        return float(v['mid'].fmeasure)\n",
    "    if hasattr(v, 'mid') and hasattr(v.mid, 'fmeasure'):\n",
    "        return float(v.mid.fmeasure)\n",
    "    return float(v)\n",
    "\n",
    "# Shared generation params (match earlier eval)\n",
    "GEN_KW = dict(max_length=30, min_length=5, num_beams=4)\n",
    "BATCH_SIZE = 8\n",
    "MAX_SAMPLES = 512\n",
    "\n",
    "# Helper to compute ROUGE for a split\n",
    "def compute_rouge_for_split(split_name: str):\n",
    "    ds = working_dataset[split_name]\n",
    "    M = min(MAX_SAMPLES, ds.num_rows)\n",
    "    samples = ds.select(range(M))\n",
    "    preds, refs = [], []\n",
    "\n",
    "    for start in tqdm(range(0, M, BATCH_SIZE), desc=f\"{split_name} gen\", leave=False):\n",
    "        end = min(start + BATCH_SIZE, M)\n",
    "        batch = samples.select(range(start, end))\n",
    "        texts = [b[\"Translated_Review\"] for b in batch]\n",
    "        ref_texts = [b[\"summary\"] for b in batch]\n",
    "        inputs = val_tokenizer([\"summarize: \" + t for t in texts], return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outs = val_model.generate(**_move_to(inputs, _device), **GEN_KW)\n",
    "        except RuntimeError as e:\n",
    "            if _device.type == 'mps':\n",
    "                with torch.no_grad():\n",
    "                    outs = val_model.to('cpu').generate(**_move_to(inputs, 'cpu'), **GEN_KW)\n",
    "                val_model.to(_device)\n",
    "            else:\n",
    "                raise\n",
    "        batch_preds = val_tokenizer.batch_decode(outs, skip_special_tokens=True)\n",
    "        preds.extend(batch_preds)\n",
    "        refs.extend(ref_texts)\n",
    "\n",
    "    rouge_res = _rouge_metric.compute(\n",
    "        predictions=[\"\\n\".join(p.split()) for p in preds],\n",
    "        references=[\"\\n\".join(r.split()) for r in refs],\n",
    "    )\n",
    "    scores = {k: _rouge_to_float(v) for k, v in rouge_res.items() if k in (\"rouge1\",\"rouge2\",\"rougeL\",\"rougeLsum\")}\n",
    "    return scores\n",
    "\n",
    "# Compute ROUGE\n",
    "train_rouge = compute_rouge_for_split('train')\n",
    "val_rouge   = compute_rouge_for_split('validation')\n",
    "\n",
    "# Compute eval losses using Trainer on each split\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "_tmp_args = TrainingArguments(output_dir=\"./_tmp_eval_compare\", per_device_eval_batch_size=8, report_to=[], logging_strategy=\"no\")\n",
    "_data_collator = DataCollatorForSeq2Seq(tokenizer=val_tokenizer, model=val_model)\n",
    "\n",
    "trainer_train_eval = Trainer(model=val_model, args=_tmp_args, eval_dataset=tokenized_dataset['train'], tokenizer=val_tokenizer, data_collator=_data_collator)\n",
    "trainer_val_eval   = Trainer(model=val_model, args=_tmp_args, eval_dataset=tokenized_dataset['validation'], tokenizer=val_tokenizer, data_collator=_data_collator)\n",
    "\n",
    "train_eval_metrics = trainer_train_eval.evaluate()\n",
    "val_eval_metrics   = trainer_val_eval.evaluate()\n",
    "\n",
    "train_loss = float(train_eval_metrics.get('eval_loss', float('nan')))\n",
    "val_loss   = float(val_eval_metrics.get('eval_loss', float('nan')))\n",
    "\n",
    "# Print comparison\n",
    "print(\"\\n=== Train vs Validation (ROUGE and eval_loss) ===\")\n",
    "print(\"Train ROUGE:\", {k: round(v, 4) for k, v in train_rouge.items()})\n",
    "print(\"Valid ROUGE:\", {k: round(v, 4) for k, v in val_rouge.items()})\n",
    "print(f\"Train eval_loss: {train_loss:.4f}\")\n",
    "print(f\"Valid eval_loss: {val_loss:.4f}\")\n",
    "\n",
    "# Quick overfitting signal\n",
    "gap_rougeL = train_rouge.get('rougeL', float('nan')) - val_rouge.get('rougeL', float('nan'))\n",
    "loss_gap   = val_loss - train_loss if (not math.isnan(train_loss) and not math.isnan(val_loss)) else float('nan')\n",
    "print(f\"\\nDelta (train - valid) ROUGE-L: {gap_rougeL:.4f}\")\n",
    "print(f\"Delta (valid - train) eval_loss: {loss_gap:.4f}\")\n",
    "if (not math.isnan(gap_rougeL) and gap_rougeL > 0.05) or (not math.isnan(loss_gap) and loss_gap > 0.3):\n",
    "    print(\"[Note] Signs of overfitting: higher train ROUGE-L and/or much lower train loss.\")\n",
    "else:\n",
    "    print(\"[Note] No strong overfitting signal based on these thresholds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Load the dataset first (cells 2–3).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Preconditions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mdf\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m(), \u001b[33m\"\u001b[39m\u001b[33mLoad the dataset first (cells 2–3).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33msummarize_app_by_sentiment\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m(), \u001b[33m\"\u001b[39m\u001b[33mDefine summarize_app_by_sentiment first (cell 10).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m_generate_prefixed\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m_SENTIMENT_PREFIX\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m(), \u001b[33m\"\u001b[39m\u001b[33mMake sure cell 10 is executed.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: Load the dataset first (cells 2–3)."
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 15. UI: Summarize any app interactively (supports unknown apps via custom reviews)\n",
    "# ==============================\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, Markdown\n",
    "except Exception as e:\n",
    "    print(\"ipywidgets not available. Install it with: pip install ipywidgets && pip install jupyterlab_widgets\")\n",
    "    raise\n",
    "\n",
    "# Preconditions\n",
    "assert 'df' in globals(), \"Load the dataset first (cells 2–3).\"\n",
    "assert 'summarize_app_by_sentiment' in globals(), \"Define summarize_app_by_sentiment first (cell 10).\"\n",
    "assert '_generate_prefixed' in globals() and '_SENTIMENT_PREFIX' in globals(), \"Make sure cell 10 is executed.\"\n",
    "\n",
    "# App options from data (sorted, unique)\n",
    "app_names = sorted(set(df['App'].astype(str).unique()))\n",
    "\n",
    "# Widgets\n",
    "w_dropdown = widgets.Dropdown(options=app_names[:5000], description='Choose:', layout=widgets.Layout(width='70%'))\n",
    "w_text = widgets.Text(description='Or type:', placeholder='Any app name...', layout=widgets.Layout(width='70%'))\n",
    "w_custom = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Optional: Paste custom reviews here, one per line.\\nIf the app is not in the CSV, these will be used to generate summaries.',\n",
    "    description='Reviews:',\n",
    "    layout=widgets.Layout(width='90%', height='120px')\n",
    ")\n",
    "w_limit = widgets.IntSlider(value=50, min=10, max=200, step=10, description='Per-sent cap:')\n",
    "w_strip = widgets.Checkbox(value=True, description='Strip sentiment prefix')\n",
    "w_button = widgets.Button(description='Summarize', button_style='primary', icon='play')\n",
    "w_out = widgets.Output(layout={'border': '1px solid #ddd'})\n",
    "\n",
    "# Handler\n",
    "\n",
    "def on_run_clicked(b):\n",
    "    with w_out:\n",
    "        w_out.clear_output()\n",
    "        name = (w_text.value or '').strip() or (w_dropdown.value or '').strip()\n",
    "        typed = (w_text.value or '').strip()\n",
    "        # Try dataset if present\n",
    "        subset = df[df['App'] == name] if name else df[df['App'] == '__NO_APP__']\n",
    "\n",
    "        if name and not subset.empty:\n",
    "            # Use dataset-backed summarization\n",
    "            counts = subset['Sentiment'].value_counts().to_dict()\n",
    "            try:\n",
    "                results = summarize_app_by_sentiment(name, per_group_limit=int(w_limit.value), strip_prefix=bool(w_strip.value))\n",
    "            except Exception as e:\n",
    "                print('Error generating summaries:', e)\n",
    "                return\n",
    "            display(Markdown(f\"## Summaries for '{name}'\\n\\nReviews: {len(subset)} | Sentiments: {counts}\"))\n",
    "            pos = results.get('Positive', '')\n",
    "            neg = results.get('Negative', '')\n",
    "            neu = results.get('Neutral', '')\n",
    "            if pos:\n",
    "                display(Markdown(f\"### Positive\\n{pos}\"))\n",
    "            if neg:\n",
    "                display(Markdown(f\"### Negative\\n{neg}\"))\n",
    "            if neu:\n",
    "                display(Markdown(f\"### Neutral\\n{neu}\"))\n",
    "        else:\n",
    "            # Fallback: use custom reviews the user pasted\n",
    "            lines = [ln.strip() for ln in (w_custom.value or '').splitlines() if ln.strip()]\n",
    "            if not lines:\n",
    "                print(\"App not found in CSV. Please paste custom reviews (one per line) in the 'Reviews' box.\")\n",
    "                return\n",
    "            # Use all pasted lines for each sentiment, forcing the sentiment prefix\n",
    "            joined = \" \\n- \".join(lines)\n",
    "            results = {}\n",
    "            for sent in [\"Positive\", \"Negative\", \"Neutral\"]:\n",
    "                forced = _SENTIMENT_PREFIX[sent]\n",
    "                try:\n",
    "                    # Reuse the lower-level generator with decoder seeding\n",
    "                    results[sent] = _generate_prefixed(joined, forced_prefix=forced, max_length=80, num_beams=4, strip_prefix=bool(w_strip.value))\n",
    "                except Exception as e:\n",
    "                    results[sent] = ''\n",
    "            label = name if name else (typed if typed else 'Custom input')\n",
    "            display(Markdown(f\"## Summaries for '{label}' (custom reviews)\\n\\nCustom reviews: {len(lines)}\"))\n",
    "            if results.get('Positive'):\n",
    "                display(Markdown(f\"### Positive\\n{results['Positive']}\"))\n",
    "            if results.get('Negative'):\n",
    "                display(Markdown(f\"### Negative\\n{results['Negative']}\"))\n",
    "            if results.get('Neutral'):\n",
    "                display(Markdown(f\"### Neutral\\n{results['Neutral']}\"))\n",
    "\n",
    "w_button.on_click(on_run_clicked)\n",
    "\n",
    "# Layout\n",
    "controls = widgets.VBox([\n",
    "    widgets.HBox([w_dropdown]),\n",
    "    widgets.HBox([w_text]),\n",
    "    widgets.HBox([w_custom]),\n",
    "    widgets.HBox([w_limit, w_strip]),\n",
    "    w_button\n",
    "])\n",
    "\n",
    "ui = widgets.VBox([controls, w_out])\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom text summaries:\n",
      "- I really love this app\n",
      "- The last update broke notifications. Please fix it soon\n",
      "- it does what it says. Nothing special, but it works\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 14. Summarize custom data with saved model/tokenizer\n",
    "# ==============================\n",
    "import os\n",
    "import re as _re\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure a default saved model dir is available\n",
    "if 'DEFAULT_SAVED_MODEL_DIR' not in globals():\n",
    "    DEFAULT_SAVED_MODEL_DIR = \\\n",
    "        \"/Users/bawantharathnayake/Desktop/Academic/semester 7/advanced ai/AI-Project/Customer-Review-Summarizer-for-Mobile-Apps-using-LLMs-Project/notebook/t5_finetuned_reviews\"\n",
    "\n",
    "# Load saved model/tokenizer if not already loaded\n",
    "\n",
    "def ensure_saved_model(saved_dir: str | None = None):\n",
    "    \"\"\"Loads val_model/val_tokenizer and device if not present; returns (model, tokenizer, device).\"\"\"\n",
    "    global val_model, val_tokenizer, _device\n",
    "    saved_dir = saved_dir or DEFAULT_SAVED_MODEL_DIR\n",
    "    if 'val_model' in globals() and 'val_tokenizer' in globals():\n",
    "        mdl, tok = val_model, val_tokenizer\n",
    "    else:\n",
    "        from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "        tok = T5Tokenizer.from_pretrained(saved_dir)\n",
    "        mdl = T5ForConditionalGeneration.from_pretrained(saved_dir)\n",
    "    _device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "    mdl.to(_device)\n",
    "    val_model = mdl\n",
    "    val_tokenizer = tok\n",
    "    return mdl, tok, _device\n",
    "\n",
    "\n",
    "def _strip_prefix(txt: str) -> str:\n",
    "    return _re.sub(r\"^(Positive feedback:|Problem:|Neutral comment:)\\s*\", \"\", str(txt)).strip()\n",
    "\n",
    "\n",
    "def summarize_texts(texts: list[str],\n",
    "                    max_input_length: int = 256,\n",
    "                    max_length: int = 64,\n",
    "                    min_length: int = 5,\n",
    "                    num_beams: int = 4,\n",
    "                    batch_size: int = 8,\n",
    "                    strip_prefix: bool = True) -> list[str]:\n",
    "    \"\"\"Summarize a list of raw texts using the saved model/tokenizer.\"\"\"\n",
    "    mdl, tok, device = ensure_saved_model()\n",
    "    outputs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        enc = tok([\"summarize: \" + t for t in batch_texts],\n",
    "                   return_tensors=\"pt\", truncation=True, padding=True, max_length=max_input_length)\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                gen = mdl.generate(**{k: v.to(device) for k, v in enc.items()},\n",
    "                                   max_length=max_length,\n",
    "                                   min_length=min_length,\n",
    "                                   num_beams=num_beams,\n",
    "                                   early_stopping=True)\n",
    "        except RuntimeError as e:\n",
    "            # MPS fallback per-batch\n",
    "            if device.type == 'mps':\n",
    "                with torch.no_grad():\n",
    "                    gen = mdl.to('cpu').generate(**{k: v.to('cpu') for k, v in enc.items()},\n",
    "                                                 max_length=max_length,\n",
    "                                                 min_length=min_length,\n",
    "                                                 num_beams=num_beams,\n",
    "                                                 early_stopping=True)\n",
    "                mdl.to(device)\n",
    "            else:\n",
    "                raise\n",
    "        dec = val_tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "        if strip_prefix:\n",
    "            dec = [_strip_prefix(x) for x in dec]\n",
    "        outputs.extend(dec)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def summarize_csv(csv_path: str,\n",
    "                   text_column: str,\n",
    "                   output_csv_path: str | None = None,\n",
    "                   max_input_length: int = 256,\n",
    "                   max_length: int = 64,\n",
    "                   min_length: int = 5,\n",
    "                   num_beams: int = 4,\n",
    "                   batch_size: int = 8,\n",
    "                   strip_prefix: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Summarize a CSV column of texts; returns a DataFrame with a new 'summary' column and optionally saves it.\"\"\"\n",
    "    df_custom = pd.read_csv(csv_path)\n",
    "    if text_column not in df_custom.columns:\n",
    "        raise ValueError(f\"Column '{text_column}' not found in {csv_path}. Columns: {list(df_custom.columns)}\")\n",
    "    texts = df_custom[text_column].astype(str).tolist()\n",
    "    df_custom['summary'] = summarize_texts(texts,\n",
    "                                          max_input_length=max_input_length,\n",
    "                                          max_length=max_length,\n",
    "                                          min_length=min_length,\n",
    "                                          num_beams=num_beams,\n",
    "                                          batch_size=batch_size,\n",
    "                                          strip_prefix=strip_prefix)\n",
    "    if output_csv_path:\n",
    "        os.makedirs(os.path.dirname(output_csv_path), exist_ok=True) if os.path.dirname(output_csv_path) else None\n",
    "        df_custom.to_csv(output_csv_path, index=False)\n",
    "        print(f\"Saved summaries to {output_csv_path}\")\n",
    "    return df_custom\n",
    "\n",
    "# --- Example usage ---\n",
    "# 1) Summarize a few custom texts\n",
    "example_texts = [\n",
    "    \"I really love this app. The interface is clean and it runs smoothly.\",\n",
    "    \"The last update broke notifications. Please fix it soon.\",\n",
    "    \"It does what it says. Nothing special, but it works.\"\n",
    "]\n",
    "example_summaries = summarize_texts(example_texts, strip_prefix=True)\n",
    "print(\"\\nCustom text summaries:\")\n",
    "for t, s in zip(example_texts, example_summaries):\n",
    "    print(\"-\", s)\n",
    "\n",
    "# 2) Summarize a CSV (uncomment and set your paths/column)\n",
    "# out_df = summarize_csv(\n",
    "#     csv_path=\"/path/to/your/custom.csv\",\n",
    "#     text_column=\"your_text_column\",\n",
    "#     output_csv_path=\"/path/to/output_summaries.csv\",\n",
    "#     strip_prefix=True,\n",
    "# )\n",
    "# out_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7140c05d294737b3dd4d1988d7ad32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Generate Synthetic App Reviews</h3><p>Creates realistic user reviews per sentim…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Synthetic review generation for any app (LLM-like)\n",
    "import random, math, re, textwrap\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, HTML, clear_output\n",
    "except Exception:\n",
    "    widgets = None\n",
    "\n",
    "_ASPECTS = [\n",
    "    \"performance\", \"speed\", \"battery usage\", \"crashes/bugs\", \"UI/UX\",\n",
    "    \"notifications\", \"privacy\", \"permissions\", \"ads\", \"pricing\",\n",
    "    \"features\", \"offline mode\", \"sync\", \"customer support\", \"updates\",\n",
    "]\n",
    "_PERSONAS = [\n",
    "    \"college student\", \"busy professional\", \"parent\", \"gamer\", \"photography enthusiast\",\n",
    "    \"traveler\", \"small business owner\", \"power user\", \"new user\", \"accessibility user\",\n",
    "]\n",
    "_STYLE_TO_LEN = {\n",
    "    \"concise\": (1, 2),\n",
    "    \"detailed\": (2, 4),\n",
    "    \"story-like\": (3, 6),\n",
    "}\n",
    "\n",
    "# Fallbacks to access a ready model/tokenizer and device\n",
    "import torch\n",
    "\n",
    "def _device_default():\n",
    "    return torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "\n",
    "\n",
    "def _get_model_tokenizer_device():\n",
    "    # T5 seq2seq (your fine-tuned model)\n",
    "    global val_model, val_tokenizer\n",
    "    model = None\n",
    "    tok = None\n",
    "    if 'val_model' in globals() and val_model is not None:\n",
    "        model = val_model\n",
    "    if 'val_tokenizer' in globals() and val_tokenizer is not None:\n",
    "        tok = val_tokenizer\n",
    "    if model is None or tok is None:\n",
    "        from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "        model_dir = None\n",
    "        if 'DEFAULT_SAVED_MODEL_DIR' in globals():\n",
    "            model_dir = DEFAULT_SAVED_MODEL_DIR\n",
    "        elif 'SAVED_MODEL_DIR' in globals():\n",
    "            model_dir = SAVED_MODEL_DIR\n",
    "        if model_dir is None:\n",
    "            model_dir = 't5-small'  # last-resort fallback\n",
    "        tok = T5Tokenizer.from_pretrained(model_dir)\n",
    "        model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
    "    device = getattr(model, 'device', None) or _device_default()\n",
    "    model.to(device).eval()\n",
    "    return model, tok, device\n",
    "\n",
    "\n",
    "# Optional causal LM for better open-ended generation\n",
    "_causal_cache = {\"model\": None, \"tok\": None, \"name\": None}\n",
    "\n",
    "def _get_causal_lm(name: str = 'distilgpt2'):\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    if _causal_cache[\"model\"] is not None and _causal_cache[\"name\"] == name:\n",
    "        return _causal_cache[\"model\"], _causal_cache[\"tok\"], _causal_cache.get(\"device\")\n",
    "    tok = AutoTokenizer.from_pretrained(name)\n",
    "    if tok.pad_token_id is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(name)\n",
    "    device = _device_default()\n",
    "    model.to(device).eval()\n",
    "    _causal_cache.update({\"model\": model, \"tok\": tok, \"name\": name, \"device\": device})\n",
    "    return model, tok, device\n",
    "\n",
    "\n",
    "# Build a targeted prompt to elicit realistic reviews\n",
    "SENTIMENT_LABELS = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "\n",
    "\n",
    "def build_review_prompt(app: str, sentiment: str, persona: str, aspects: List[str], style: str) -> str:\n",
    "    # Minimal, content-only context to avoid instruction echo\n",
    "    min_s, max_s = _STYLE_TO_LEN.get(style, (1, 2))\n",
    "    return \"\\n\".join([\n",
    "        f\"App: {app}\",\n",
    "        f\"Persona: {persona}\",\n",
    "        f\"Sentiment: {sentiment}\",\n",
    "        f\"Aspects: {', '.join(aspects)}\",\n",
    "        f\"Sentences: {min_s}-{max_s}\",\n",
    "        \"Review:\",\n",
    "    ])\n",
    "\n",
    "\n",
    "def _strip_any_prefix(text: str) -> str:\n",
    "    # Remove known sentiment prefixes if model adds them\n",
    "    try:\n",
    "        prefixes = list(SENTIMENT_TO_PREFIX.values()) if 'SENTIMENT_TO_PREFIX' in globals() else []\n",
    "    except Exception:\n",
    "        prefixes = []\n",
    "    t = text.strip()\n",
    "    for p in prefixes:\n",
    "        if t.lower().startswith(p.lower()):\n",
    "            t = t[len(p):].lstrip(\" -:\\n\\t\")\n",
    "    # remove generic labels like 'Review:' at start\n",
    "    t = re.sub(r'^(review\\s*[:\\-]\\s*)', '', t, flags=re.IGNORECASE).strip()\n",
    "    # collapse spaces\n",
    "    t = re.sub(r'\\s+', ' ', t).strip()\n",
    "    return t\n",
    "\n",
    "\n",
    "def _seed_text(persona: str, sentiment: str, app: str, aspects: List[str]) -> str:\n",
    "    s = sentiment.strip().lower()\n",
    "    aspects_txt = ', '.join(aspects)\n",
    "    if s.startswith('pos'):\n",
    "        return f\"As a {persona}, I love how {app} handles {aspects_txt}. \"\n",
    "    if s.startswith('neg'):\n",
    "        return f\"As a {persona}, I'm frustrated with {app}, especially around {aspects_txt}. \"\n",
    "    return f\"As a {persona}, I find {app} okay overall for {aspects_txt}. \"\n",
    "\n",
    "\n",
    "def _finish_sentence(text: str) -> str:\n",
    "    # Trim to last sentence end if it exists\n",
    "    m = re.search(r'[\\.\\!\\?](?!.*[\\.\\!\\?])', text)\n",
    "    if m:\n",
    "        return text[:m.end()].strip()\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def generate_reviews_for_app(\n",
    "    app: str,\n",
    "    n_per_sentiment: int = 3,\n",
    "    sentiments: List[str] = None,\n",
    "    style: str = \"concise\",\n",
    "    temperature: float = 0.9,\n",
    "    top_p: float = 0.92,\n",
    "    max_new_tokens: int = 72,\n",
    "    min_new_tokens: int = 12,\n",
    "    seed_with_prefix: bool = True,\n",
    "    backend: str = 'auto',  # 'auto' | 't5' | 'gpt2'\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Generate synthetic user reviews per sentiment for an app.\n",
    "\n",
    "    backend:\n",
    "      - 't5': use your fine-tuned T5 (may echo instruction-like text)\n",
    "      - 'gpt2': use a small causal LM for open-ended text\n",
    "      - 'auto': prefer gpt2 if available, else fallback to T5\n",
    "    \"\"\"\n",
    "    if sentiments is None:\n",
    "        sentiments = SENTIMENT_LABELS\n",
    "\n",
    "    use_gpt2 = backend == 'gpt2'\n",
    "    if backend == 'auto':\n",
    "        try:\n",
    "            _ = _get_causal_lm('distilgpt2')\n",
    "            use_gpt2 = True\n",
    "        except Exception:\n",
    "            use_gpt2 = False\n",
    "\n",
    "    out: Dict[str, List[str]] = {s: [] for s in sentiments}\n",
    "\n",
    "    if use_gpt2:\n",
    "        gpt2, tok, device = _get_causal_lm('distilgpt2')\n",
    "        gen_kw = dict(\n",
    "            do_sample=True,\n",
    "            top_p=float(top_p),\n",
    "            temperature=float(temperature),\n",
    "            max_new_tokens=int(max_new_tokens),\n",
    "            repetition_penalty=1.05,\n",
    "            no_repeat_ngram_size=3,\n",
    "            pad_token_id=tok.pad_token_id,\n",
    "            eos_token_id=tok.eos_token_id,\n",
    "        )\n",
    "        for s in sentiments:\n",
    "            for _ in range(int(n_per_sentiment)):\n",
    "                aspects = random.sample(_ASPECTS, k=min(2, len(_ASPECTS)))\n",
    "                persona = random.choice(_PERSONAS)\n",
    "                seed = _seed_text(persona, s, app, aspects)\n",
    "                prompt = seed  # causal LM prompt is the seed itself\n",
    "                enc = tok(prompt, return_tensors='pt').to(device)\n",
    "                with torch.no_grad():\n",
    "                    out_ids = gpt2.generate(**enc, **gen_kw)\n",
    "                txt = tok.decode(out_ids[0], skip_special_tokens=True)\n",
    "                # remove the seed if it got copied fully\n",
    "                gen_part = txt[len(prompt):].strip() if txt.startswith(prompt) else txt\n",
    "                final = _finish_sentence((seed + gen_part).strip())\n",
    "                out[s].append(final)\n",
    "        return out\n",
    "\n",
    "    # T5 path (seq2seq)\n",
    "    model, tok, device = _get_model_tokenizer_device()\n",
    "    gen_kw = dict(\n",
    "        do_sample=True,\n",
    "        top_p=float(top_p),\n",
    "        temperature=float(temperature),\n",
    "        max_new_tokens=int(max_new_tokens),\n",
    "        min_new_tokens=int(min_new_tokens),\n",
    "        pad_token_id=tok.pad_token_id,\n",
    "        eos_token_id=tok.eos_token_id,\n",
    "        repetition_penalty=1.05,\n",
    "        no_repeat_ngram_size=3,\n",
    "    )\n",
    "\n",
    "    for s in sentiments:\n",
    "        for _ in range(int(n_per_sentiment)):\n",
    "            aspects = random.sample(_ASPECTS, k=min(2, len(_ASPECTS)))\n",
    "            persona = random.choice(_PERSONAS)\n",
    "            prompt = build_review_prompt(app, s, persona, aspects, style)\n",
    "            enc = tok(prompt, return_tensors='pt', truncation=True, max_length=256)\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "            dec_args = {}\n",
    "            if seed_with_prefix:\n",
    "                seed_txt = _seed_text(persona, s, app, aspects)\n",
    "                dec_ids = tok(seed_txt, return_tensors='pt', add_special_tokens=False).input_ids.to(device)\n",
    "                dec_args['decoder_input_ids'] = dec_ids\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out_ids = model.generate(**enc, **dec_args, **gen_kw)\n",
    "            txt = tok.decode(out_ids[0], skip_special_tokens=True)\n",
    "            txt = _strip_any_prefix(txt)\n",
    "            if not txt:\n",
    "                txt = seed_txt  # ensure some content\n",
    "            words = txt.split()\n",
    "            if len(words) < max(5, min_new_tokens // 2):\n",
    "                with torch.no_grad():\n",
    "                    out_ids = model.generate(**enc, **dec_args, **gen_kw)\n",
    "                txt = tok.decode(out_ids[0], skip_special_tokens=True)\n",
    "                txt = _strip_any_prefix(txt)\n",
    "            out[s].append(_finish_sentence(txt))\n",
    "    return out\n",
    "\n",
    "\n",
    "def _render_synth_output(reviews: Dict[str, List[str]]):\n",
    "    lines = []\n",
    "    for s in reviews:\n",
    "        lines.append(f\"\\n=== {s} Reviews ===\")\n",
    "        for i, r in enumerate(reviews[s], 1):\n",
    "            lines.append(f\"{i}. {r}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# --- Optional UI ---\n",
    "if widgets is not None:\n",
    "    w2_app = widgets.Text(description='App', placeholder='Enter app name')\n",
    "    w2_backend = widgets.Dropdown(description='Backend', options=['auto','gpt2','t5'], value='auto')\n",
    "    w2_n = widgets.IntSlider(description='#/sent', value=3, min=1, max=6, step=1)\n",
    "    w2_style = widgets.Dropdown(description='Style', options=list(_STYLE_TO_LEN.keys()), value='concise')\n",
    "    w2_sent = widgets.SelectMultiple(description='Sentiments', options=SENTIMENT_LABELS, value=tuple(SENTIMENT_LABELS))\n",
    "    w2_temp = widgets.FloatSlider(description='Temp', value=0.9, min=0.1, max=1.5, step=0.05)\n",
    "    w2_top_p = widgets.FloatSlider(description='Top-p', value=0.92, min=0.1, max=1.0, step=0.02)\n",
    "    w2_len = widgets.IntRangeSlider(description='New tokens', value=[12, 72], min=8, max=160, step=4)\n",
    "    w2_seed = widgets.Checkbox(description='Seed persona', value=True)\n",
    "    w2_btn = widgets.Button(description='Run', button_style='primary')\n",
    "    w2_out = widgets.Output(layout={'border': '1px solid #ddd'})\n",
    "\n",
    "    def _on_run(_):\n",
    "        w2_out.clear_output()\n",
    "        with w2_out:\n",
    "            app = w2_app.value.strip()\n",
    "            if not app:\n",
    "                print('Please enter an app name.')\n",
    "                return\n",
    "            res = generate_reviews_for_app(\n",
    "                app=app,\n",
    "                n_per_sentiment=w2_n.value,\n",
    "                sentiments=list(w2_sent.value),\n",
    "                style=w2_style.value,\n",
    "                temperature=w2_temp.value,\n",
    "                top_p=w2_top_p.value,\n",
    "                min_new_tokens=w2_len.value[0],\n",
    "                max_new_tokens=w2_len.value[1],\n",
    "                seed_with_prefix=w2_seed.value,\n",
    "                backend=w2_backend.value,\n",
    "            )\n",
    "            print(_render_synth_output(res))\n",
    "\n",
    "    w2_btn.on_click(_on_run)\n",
    "    ui_gen = widgets.VBox([\n",
    "        widgets.HTML('<h3>Generate Synthetic App Reviews</h3><p>Creates realistic user reviews per sentiment for any app. Backend auto=GPT-2 fallback.</p>'),\n",
    "        widgets.HBox([w2_app, w2_backend, w2_style]),\n",
    "        widgets.HBox([w2_n, w2_sent]),\n",
    "        widgets.HBox([w2_temp, w2_top_p, w2_len]),\n",
    "        widgets.HBox([w2_seed]),\n",
    "        w2_btn,\n",
    "        w2_out\n",
    "    ])\n",
    "    display(ui_gen)\n",
    "else:\n",
    "    print(\"ipywidgets not available. Use generate_reviews_for_app(app='Your App') programmatically.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: As a gamer, I love how TestApp handles offline mode, features. ????\n",
      "Negative: As a busy professional, I'm frustrated with TestApp, especially around customer support, pricing. So do all my friends here and our associates: Do you think if they can help in sales? If so, or are there any tips for getting over your head about this problem? Let's talk to someone that says hi! We're looking forward to hearing from you!\n",
      "Neutral: As a college student, I find TestApp okay overall for battery usage, offline mode.  - The power and battery life of these two Android tablets may look decent when it comes to battery storage and performance.\n"
     ]
    }
   ],
   "source": [
    "# Smoke test: ensure generator produces new reviews\n",
    "try:\n",
    "    _test = generate_reviews_for_app(\n",
    "        app=\"TestApp\",\n",
    "        n_per_sentiment=1,\n",
    "        temperature=1.1,\n",
    "        top_p=0.95,\n",
    "        max_new_tokens=80,\n",
    "        min_new_tokens=20,\n",
    "        seed_with_prefix=True,\n",
    "    )\n",
    "    for s, lst in _test.items():\n",
    "        print(f\"{s}: {lst[0]}\")\n",
    "except Exception as e:\n",
    "    print('Generation error:', e)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "010948da568f46fe99169d5f0379f93e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04333f12f4654045b1b0e42717cf42b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04630089defc42efb25408e106869926": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "06aba812ce044c86958e9ab0859224c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "06b75bb53d28485794babe3bbb1b02dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "090639f80d5a49bc810bbe1c037b44a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0bde9ebb876043c4a42dfd7e11288bbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0ca74ddf1bbd445489f978347ef0989f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1abd1752030f4cbba2d059dc0689be0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1bef6286598d401fa4df8c432dbe484e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cce6ebd5dea74d7d8333fbdac2e6ebd9",
      "placeholder": "​",
      "style": "IPY_MODEL_3aa7b433b7be454f80f68c191501919e",
      "value": " 147/147 [00:00&lt;00:00, 2.21kB/s]"
     }
    },
    "2272cecf9e714ceab1186dd9ea811404": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2a213450041b4651b8a0a39a57205663",
       "IPY_MODEL_a5c0347c081b4d9fb771d93fa10a2c63",
       "IPY_MODEL_d7155b2edfe14c8f87f0691bb5db2bb9"
      ],
      "layout": "IPY_MODEL_cd4ecbe10fee42349aa11e2fa117719a"
     }
    },
    "24f6106dfc74467596379cfc2432d6db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04333f12f4654045b1b0e42717cf42b7",
      "placeholder": "​",
      "style": "IPY_MODEL_758ffaa17c44431092749cff9a92deec",
      "value": "tokenizer.json: 100%"
     }
    },
    "28db633d8e1142b693e749ec62ce126a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2a213450041b4651b8a0a39a57205663": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_090639f80d5a49bc810bbe1c037b44a1",
      "placeholder": "​",
      "style": "IPY_MODEL_9a7c9d52e1db4acfbdc4d4a036f4743e",
      "value": "model.safetensors: 100%"
     }
    },
    "2ad2ed2298fc4db9b968de1322c342da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e7a3a68d2e340b3a49dc567cd371262": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "316cc7242c844bb288b8fe0292d3e113": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31c6236109604626a3ce60b9bc026812": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae43045ba91f4de18d6357d4264ff165",
      "placeholder": "​",
      "style": "IPY_MODEL_ff1951fa3f5646bf895d6b679dbb364e",
      "value": "Downloading builder script: "
     }
    },
    "33764fc5c7de4dd3aacf9637cb066c83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "387af6ad11f44001b7523913861f570d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_653f21e9eab443c8bc404aab4a4677e5",
      "max": 2324,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_de820d723cdc46dbaed20894b9131be5",
      "value": 2324
     }
    },
    "3aa7b433b7be454f80f68c191501919e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3ae0e01614f84605a807ea6cae9a8381": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "403a777bea9c4eafae3f1bb369d557c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "41b4fa16c9f6406882dc7257ca71008a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa997041eda34d538e9237326e2bf57d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_45d3ae4c802a4076a670653aef5fbec2",
      "value": 1
     }
    },
    "45d3ae4c802a4076a670653aef5fbec2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "48d84c5c9a1b40639b644d75224fd1c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4e244b5f61e14713a25243f3cb4f8e89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "517a73eda7774b00bfa6b80586c4a851": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "55461054a7214ccc9f670e2310f2dd26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6683f328eb85489190d9987de7795296",
      "max": 7486,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3ae0e01614f84605a807ea6cae9a8381",
      "value": 7486
     }
    },
    "56679a36e96e40a584dd9b463fc92ed2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_24f6106dfc74467596379cfc2432d6db",
       "IPY_MODEL_7b1d92efa26a42b897601194d032dc18",
       "IPY_MODEL_ee986d5e1071420b8a9f38aae7199576"
      ],
      "layout": "IPY_MODEL_ab950ce8a7884d7190b0eca5dff97a0d"
     }
    },
    "56d115c86ec04bcf9024e8c570bbbc7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06b75bb53d28485794babe3bbb1b02dc",
      "placeholder": "​",
      "style": "IPY_MODEL_cfbba8554000456ab856e9be1eb5a5c2",
      "value": "Map: 100%"
     }
    },
    "58268c97e0644fc1a568e5702e116028": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3865225ab254132b6058c7b790824b5",
      "placeholder": "​",
      "style": "IPY_MODEL_0ca74ddf1bbd445489f978347ef0989f",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "5cfd31766a934c578b62b891a836500d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_58268c97e0644fc1a568e5702e116028",
       "IPY_MODEL_387af6ad11f44001b7523913861f570d",
       "IPY_MODEL_c285e5ed0e48436fab20ce6b7aa4588e"
      ],
      "layout": "IPY_MODEL_a2b3a6c4a711466e8f39f780228ac554"
     }
    },
    "60a7bb70698747bba668eb72b6026b02": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "653f21e9eab443c8bc404aab4a4677e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6683f328eb85489190d9987de7795296": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ba7112ae05e45feacfe013937940e74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f376a813886473583a544429aa13cdd",
      "placeholder": "​",
      "style": "IPY_MODEL_2e7a3a68d2e340b3a49dc567cd371262",
      "value": " 6.27k/? [00:00&lt;00:00, 75.8kB/s]"
     }
    },
    "709f2946b7354f9dade6b4ae81da96d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7116a0ad2deb4ef4bb2b8fd1787432ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "758ffaa17c44431092749cff9a92deec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7b1d92efa26a42b897601194d032dc18": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c8302396d1c4b21ace3016ad694a1e2",
      "max": 1389353,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_be2432abb4f14f1c8738e3f12b868866",
      "value": 1389353
     }
    },
    "7c173d6002e24debb28f67e296d7f1c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d783b6049534edc950be6dafe6d8c4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe53d8854010425c85fe3cd2d8c57280",
      "max": 147,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c95b1d4a560e4f44b97f5be9580e81bd",
      "value": 147
     }
    },
    "816822b41a4e4dfca05d56f26adf417d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_56d115c86ec04bcf9024e8c570bbbc7e",
       "IPY_MODEL_f2c4099d567e4e15a607e94f5516c8c7",
       "IPY_MODEL_87142a13db694c9e87c9429baf3d9a0a"
      ],
      "layout": "IPY_MODEL_8c195248fcf84980ab795ba63b58b20c"
     }
    },
    "84cb3f791d8242a9b8e4d28626086e14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_31c6236109604626a3ce60b9bc026812",
       "IPY_MODEL_41b4fa16c9f6406882dc7257ca71008a",
       "IPY_MODEL_6ba7112ae05e45feacfe013937940e74"
      ],
      "layout": "IPY_MODEL_d89b762f7d914258a80c284817eeafb3"
     }
    },
    "87142a13db694c9e87c9429baf3d9a0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93bfbb2eb49a4305b8e96c4edcf881c8",
      "placeholder": "​",
      "style": "IPY_MODEL_517a73eda7774b00bfa6b80586c4a851",
      "value": " 29941/29941 [00:42&lt;00:00, 896.08 examples/s]"
     }
    },
    "8c195248fcf84980ab795ba63b58b20c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8eddee38fd734e00971f3208cde9ed22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9097cbfa99654e22a2d604a86d244311": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "925f16cf7417444c91eb82f68ecf5075": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_010948da568f46fe99169d5f0379f93e",
      "placeholder": "​",
      "style": "IPY_MODEL_9097cbfa99654e22a2d604a86d244311",
      "value": "generation_config.json: 100%"
     }
    },
    "93bfbb2eb49a4305b8e96c4edcf881c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98080670683c4a26a7a44d00f71fdb00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c123eef68e42427aa47c0b6424363e9f",
      "placeholder": "​",
      "style": "IPY_MODEL_1abd1752030f4cbba2d059dc0689be0e",
      "value": " 1.21k/1.21k [00:00&lt;00:00, 23.3kB/s]"
     }
    },
    "99e861d7394246ceb857408755e71d52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_316cc7242c844bb288b8fe0292d3e113",
      "max": 791656,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0bde9ebb876043c4a42dfd7e11288bbc",
      "value": 791656
     }
    },
    "9a7c9d52e1db4acfbdc4d4a036f4743e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c05752eaf8d48439b81de53f64b295d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cc61309445ef4c9180b892415ac2ad96",
       "IPY_MODEL_55461054a7214ccc9f670e2310f2dd26",
       "IPY_MODEL_f3ba980d246a428e9e891434ff9358cc"
      ],
      "layout": "IPY_MODEL_7116a0ad2deb4ef4bb2b8fd1787432ee"
     }
    },
    "9c8302396d1c4b21ace3016ad694a1e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f376a813886473583a544429aa13cdd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f759eb130564255b751a3efad60ce7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a2b3a6c4a711466e8f39f780228ac554": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5c0347c081b4d9fb771d93fa10a2c63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8eddee38fd734e00971f3208cde9ed22",
      "max": 242043056,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9f759eb130564255b751a3efad60ce7a",
      "value": 242043056
     }
    },
    "ab950ce8a7884d7190b0eca5dff97a0d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae43045ba91f4de18d6357d4264ff165": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aef51e2e37f84e78b6529280cccd860d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b5765f81572d4b8199f8510d4427a55f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc0a5e3c2c664028ae059d2ea5ecd99c",
      "placeholder": "​",
      "style": "IPY_MODEL_48d84c5c9a1b40639b644d75224fd1c6",
      "value": "config.json: 100%"
     }
    },
    "be2432abb4f14f1c8738e3f12b868866": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c08b45d3812340e8bf4deeb225b973b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c123eef68e42427aa47c0b6424363e9f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c285e5ed0e48436fab20ce6b7aa4588e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04630089defc42efb25408e106869926",
      "placeholder": "​",
      "style": "IPY_MODEL_4e244b5f61e14713a25243f3cb4f8e89",
      "value": " 2.32k/2.32k [00:00&lt;00:00, 49.6kB/s]"
     }
    },
    "c4173c1abcd447d88422fdd5535f3cea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c758d07be4f5405e8e3d31573cb8cf7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_925f16cf7417444c91eb82f68ecf5075",
       "IPY_MODEL_7d783b6049534edc950be6dafe6d8c4f",
       "IPY_MODEL_1bef6286598d401fa4df8c432dbe484e"
      ],
      "layout": "IPY_MODEL_d037d570f875482e98ea898a2734b8f0"
     }
    },
    "c7818e7fabb64119afa28d76878f1b17": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c95b1d4a560e4f44b97f5be9580e81bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cc61309445ef4c9180b892415ac2ad96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ad2ed2298fc4db9b968de1322c342da",
      "placeholder": "​",
      "style": "IPY_MODEL_dac72fd25316466995e17eebdef02742",
      "value": "Map: 100%"
     }
    },
    "cce6ebd5dea74d7d8333fbdac2e6ebd9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd4ecbe10fee42349aa11e2fa117719a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfbba8554000456ab856e9be1eb5a5c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d037d570f875482e98ea898a2734b8f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1b5ccbaac3648ac823104269ab1ed7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d45f03336d544703aaeef0799f00e97e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fb40717576bc4d1d962644c7cb82ee79",
       "IPY_MODEL_99e861d7394246ceb857408755e71d52",
       "IPY_MODEL_fc6e617c08b148d1b8cf90530b67fc7c"
      ],
      "layout": "IPY_MODEL_fe60d6ff5259411280f2690b75b0cbc2"
     }
    },
    "d7155b2edfe14c8f87f0691bb5db2bb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da58ee18c02f4bed8890f3784942f572",
      "placeholder": "​",
      "style": "IPY_MODEL_aef51e2e37f84e78b6529280cccd860d",
      "value": " 242M/242M [00:04&lt;00:00, 64.0MB/s]"
     }
    },
    "d89b762f7d914258a80c284817eeafb3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da58ee18c02f4bed8890f3784942f572": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dac72fd25316466995e17eebdef02742": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc0a5e3c2c664028ae059d2ea5ecd99c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de820d723cdc46dbaed20894b9131be5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "df7c43c1eb4e43ce934895e772fb3d76": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1736427ca6d4ad887ff5e9372819b79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c08b45d3812340e8bf4deeb225b973b5",
      "max": 1206,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_28db633d8e1142b693e749ec62ce126a",
      "value": 1206
     }
    },
    "e3865225ab254132b6058c7b790824b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee986d5e1071420b8a9f38aae7199576": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7818e7fabb64119afa28d76878f1b17",
      "placeholder": "​",
      "style": "IPY_MODEL_d1b5ccbaac3648ac823104269ab1ed7f",
      "value": " 1.39M/1.39M [00:00&lt;00:00, 11.1MB/s]"
     }
    },
    "f2c4099d567e4e15a607e94f5516c8c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06aba812ce044c86958e9ab0859224c0",
      "max": 29941,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_403a777bea9c4eafae3f1bb369d557c8",
      "value": 29941
     }
    },
    "f3ba980d246a428e9e891434ff9358cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c173d6002e24debb28f67e296d7f1c1",
      "placeholder": "​",
      "style": "IPY_MODEL_f4d6049fef3a466fa8c482934c1243ef",
      "value": " 7486/7486 [00:08&lt;00:00, 1051.75 examples/s]"
     }
    },
    "f46bba77758c4336a6ba26c1d659b787": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b5765f81572d4b8199f8510d4427a55f",
       "IPY_MODEL_e1736427ca6d4ad887ff5e9372819b79",
       "IPY_MODEL_98080670683c4a26a7a44d00f71fdb00"
      ],
      "layout": "IPY_MODEL_60a7bb70698747bba668eb72b6026b02"
     }
    },
    "f4d6049fef3a466fa8c482934c1243ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa997041eda34d538e9237326e2bf57d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "fb40717576bc4d1d962644c7cb82ee79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_709f2946b7354f9dade6b4ae81da96d5",
      "placeholder": "​",
      "style": "IPY_MODEL_c4173c1abcd447d88422fdd5535f3cea",
      "value": "spiece.model: 100%"
     }
    },
    "fc6e617c08b148d1b8cf90530b67fc7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df7c43c1eb4e43ce934895e772fb3d76",
      "placeholder": "​",
      "style": "IPY_MODEL_33764fc5c7de4dd3aacf9637cb066c83",
      "value": " 792k/792k [00:00&lt;00:00, 3.86MB/s]"
     }
    },
    "fe53d8854010425c85fe3cd2d8c57280": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe60d6ff5259411280f2690b75b0cbc2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff1951fa3f5646bf895d6b679dbb364e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
